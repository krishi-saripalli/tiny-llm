============================= test session starts ==============================
platform darwin -- Python 3.12.11, pytest-8.4.1, pluggy-1.5.0 -- /Users/krishisaripalli/misc/tiny-llm/.venv/bin/python
cachedir: .pytest_cache
benchmark: 5.1.0 (defaults: timer=time.perf_counter disable_gc=False min_rounds=5 min_time=0.000005 max_time=1.0 calibration_precision=10 warmup=False warmup_iterations=100000)
rootdir: /Users/krishisaripalli/misc/tiny-llm
configfile: pyproject.toml
plugins: benchmark-5.1.0
collecting ... collected 36 items / 8 deselected / 28 selected

tests/test_week_1_day_1.py::test_task_1_softmax[f32-cpu] PASSED          [  3%]
tests/test_week_1_day_1.py::test_task_1_softmax[f32-gpu] PASSED          [  7%]
tests/test_week_1_day_1.py::test_task_1_softmax[f16-cpu] PASSED          [ 10%]
tests/test_week_1_day_1.py::test_task_1_softmax[f16-gpu] PASSED          [ 14%]
tests/test_week_1_day_1.py::test_task_1_simple_attention[batch_0-f32-cpu] FAILED [ 17%]
tests/test_week_1_day_1.py::test_task_1_simple_attention[batch_0-f32-gpu] FAILED [ 21%]
tests/test_week_1_day_1.py::test_task_1_simple_attention[batch_0-f16-cpu] FAILED [ 25%]
tests/test_week_1_day_1.py::test_task_1_simple_attention[batch_0-f16-gpu] FAILED [ 28%]
tests/test_week_1_day_1.py::test_task_1_simple_attention[batch_1-f32-cpu] FAILED [ 32%]
tests/test_week_1_day_1.py::test_task_1_simple_attention[batch_1-f32-gpu] FAILED [ 35%]
tests/test_week_1_day_1.py::test_task_1_simple_attention[batch_1-f16-cpu] FAILED [ 39%]
tests/test_week_1_day_1.py::test_task_1_simple_attention[batch_1-f16-gpu] FAILED [ 42%]
tests/test_week_1_day_1.py::test_task_1_simple_attention[batch_2-f32-cpu] FAILED [ 46%]
tests/test_week_1_day_1.py::test_task_1_simple_attention[batch_2-f32-gpu] FAILED [ 50%]
tests/test_week_1_day_1.py::test_task_1_simple_attention[batch_2-f16-cpu] FAILED [ 53%]
tests/test_week_1_day_1.py::test_task_1_simple_attention[batch_2-f16-gpu] FAILED [ 57%]
tests/test_week_1_day_1.py::test_task_1_simple_attention_scale_mask[batch_0-f32-cpu] FAILED [ 60%]
tests/test_week_1_day_1.py::test_task_1_simple_attention_scale_mask[batch_0-f32-gpu] FAILED [ 64%]
tests/test_week_1_day_1.py::test_task_1_simple_attention_scale_mask[batch_0-f16-cpu] FAILED [ 67%]
tests/test_week_1_day_1.py::test_task_1_simple_attention_scale_mask[batch_0-f16-gpu] FAILED [ 71%]
tests/test_week_1_day_1.py::test_task_1_simple_attention_scale_mask[batch_1-f32-cpu] FAILED [ 75%]
tests/test_week_1_day_1.py::test_task_1_simple_attention_scale_mask[batch_1-f32-gpu] FAILED [ 78%]
tests/test_week_1_day_1.py::test_task_1_simple_attention_scale_mask[batch_1-f16-cpu] FAILED [ 82%]
tests/test_week_1_day_1.py::test_task_1_simple_attention_scale_mask[batch_1-f16-gpu] FAILED [ 85%]
tests/test_week_1_day_1.py::test_task_1_simple_attention_scale_mask[batch_2-f32-cpu] FAILED [ 89%]
tests/test_week_1_day_1.py::test_task_1_simple_attention_scale_mask[batch_2-f32-gpu] FAILED [ 92%]
tests/test_week_1_day_1.py::test_task_1_simple_attention_scale_mask[batch_2-f16-cpu] FAILED [ 96%]
tests/test_week_1_day_1.py::test_task_1_simple_attention_scale_mask[batch_2-f16-gpu] FAILED [100%]

=================================== FAILURES ===================================
________________ test_task_1_simple_attention[batch_0-f32-cpu] _________________

stream = DeviceType.cpu, precision = mlx.core.float32, batch_dimension = 0

    @pytest.mark.parametrize("stream", AVAILABLE_STREAMS, ids=AVAILABLE_STREAMS_IDS)
    @pytest.mark.parametrize("precision", PRECISIONS, ids=PRECISION_IDS)
    @pytest.mark.parametrize(
        "batch_dimension", [0, 1, 2], ids=["batch_0", "batch_1", "batch_2"]
    )
    def test_task_1_simple_attention(
        stream: mx.Stream, precision: mx.Dtype, batch_dimension: int
    ):
        """
        Test if `scaled_dot_product_attention_simple` can process Q/K/V correctly.
        We assume Q/K/V are of the same dimensions and test different batch dimensions.
        """
        with mx.stream(stream):
            if batch_dimension == 0:
                BATCH_SIZE = ()
            elif batch_dimension == 1:
                BATCH_SIZE = (2, 3)
            elif batch_dimension == 2:
                BATCH_SIZE = (2, 3, 3)
            DIM_L = 4
            DIM_D = 5
            for _ in range(100):
                query = mx.random.uniform(
                    shape=(*BATCH_SIZE, DIM_L, DIM_D), dtype=precision
                )
                key = mx.random.uniform(shape=(*BATCH_SIZE, DIM_L, DIM_D), dtype=precision)
                value = mx.random.uniform(
                    shape=(*BATCH_SIZE, DIM_L, DIM_D), dtype=precision
                )
                reference_output = mx.fast.scaled_dot_product_attention(
                    q=query.reshape(1, -1, DIM_L, DIM_D),
                    k=key.reshape(1, -1, DIM_L, DIM_D),
                    v=value.reshape(1, -1, DIM_L, DIM_D),
                    scale=1.0 / (DIM_D**0.5),
                ).reshape(*BATCH_SIZE, DIM_L, DIM_D)
>               user_output = scaled_dot_product_attention_simple(
                    query,
                    key,
                    value,
                )

tests/test_week_1_day_1.py:56: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

query = array([[0.324045, 0.588493, 0.0197059, 0.84507, 0.181061],
       [0.254772, 0.944777, 0.191268, 0.537365, 0.167245],
...73, 0.900003, 0.376419, 0.772495, 0.406672],
       [0.913616, 0.671529, 0.472432, 0.358955, 0.660309]], dtype=float32)
key = array([[0.138032, 0.583386, 0.797229, 0.344014, 0.349943],
       [0.756879, 0.0959947, 0.133929, 0.194891, 0.608422],... 0.00782906, 0.35601, 0.500662, 0.884273],
       [0.0327318, 0.0278976, 0.954545, 0.801391, 0.688249]], dtype=float32)
value = array([[0.621998, 0.86866, 0.568796, 0.686145, 0.196774],
       [0.700477, 0.662645, 0.122339, 0.567117, 0.374894],
 ...7754, 0.944293, 0.30289, 0.042452, 0.895055],
       [0.749926, 0.694295, 0.330487, 0.797637, 0.64839]], dtype=float32)
scale = array(0.447214, dtype=float32), mask = None

    def scaled_dot_product_attention_simple(
        query: mx.array,
        key: mx.array,
        value: mx.array,
        scale: float | None = None,
        mask: mx.array | None = None,
    ) -> mx.array:
    
        dot_prods = query @ key.swapaxes(-2,-1) # (.., Lq, Lk)
        d_k = key.shape[-1]
        scale = scale if scale is not None else 1.0 / mx.sqrt(d_k)
        dot_prods *= scale
    
>       assert(mask.shape == dot_prods.shape)
               ^^^^^^^^^^
E       AttributeError: 'NoneType' object has no attribute 'shape'

src/tiny_llm/attention.py:26: AttributeError
________________ test_task_1_simple_attention[batch_0-f32-gpu] _________________

stream = DeviceType.gpu, precision = mlx.core.float32, batch_dimension = 0

    @pytest.mark.parametrize("stream", AVAILABLE_STREAMS, ids=AVAILABLE_STREAMS_IDS)
    @pytest.mark.parametrize("precision", PRECISIONS, ids=PRECISION_IDS)
    @pytest.mark.parametrize(
        "batch_dimension", [0, 1, 2], ids=["batch_0", "batch_1", "batch_2"]
    )
    def test_task_1_simple_attention(
        stream: mx.Stream, precision: mx.Dtype, batch_dimension: int
    ):
        """
        Test if `scaled_dot_product_attention_simple` can process Q/K/V correctly.
        We assume Q/K/V are of the same dimensions and test different batch dimensions.
        """
        with mx.stream(stream):
            if batch_dimension == 0:
                BATCH_SIZE = ()
            elif batch_dimension == 1:
                BATCH_SIZE = (2, 3)
            elif batch_dimension == 2:
                BATCH_SIZE = (2, 3, 3)
            DIM_L = 4
            DIM_D = 5
            for _ in range(100):
                query = mx.random.uniform(
                    shape=(*BATCH_SIZE, DIM_L, DIM_D), dtype=precision
                )
                key = mx.random.uniform(shape=(*BATCH_SIZE, DIM_L, DIM_D), dtype=precision)
                value = mx.random.uniform(
                    shape=(*BATCH_SIZE, DIM_L, DIM_D), dtype=precision
                )
                reference_output = mx.fast.scaled_dot_product_attention(
                    q=query.reshape(1, -1, DIM_L, DIM_D),
                    k=key.reshape(1, -1, DIM_L, DIM_D),
                    v=value.reshape(1, -1, DIM_L, DIM_D),
                    scale=1.0 / (DIM_D**0.5),
                ).reshape(*BATCH_SIZE, DIM_L, DIM_D)
>               user_output = scaled_dot_product_attention_simple(
                    query,
                    key,
                    value,
                )

tests/test_week_1_day_1.py:56: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

query = array([[0.0580481, 0.121337, 0.274021, 0.999225, 0.298832],
       [0.616231, 0.513406, 0.971423, 0.424247, 0.361962],...66, 0.966035, 0.580568, 0.370206, 0.120844],
       [0.0401563, 0.987114, 0.5458, 0.893975, 0.0919037]], dtype=float32)
key = array([[0.594331, 0.701394, 0.867576, 0.315987, 0.229108],
       [0.040221, 0.264821, 0.0957578, 0.212027, 0.721296],...1, 0.630169, 0.195968, 0.96646, 0.0650969],
       [0.209112, 0.210142, 0.949457, 0.0777269, 0.408857]], dtype=float32)
value = array([[0.224786, 0.741653, 0.62667, 0.125038, 0.301318],
       [0.117694, 0.319934, 0.165849, 0.905347, 0.320611],
 ...3208, 0.515392, 0.923215, 0.367889, 0.473831],
       [0.466157, 0.307951, 0.155783, 0.6075, 0.262324]], dtype=float32)
scale = array(0.447214, dtype=float32), mask = None

    def scaled_dot_product_attention_simple(
        query: mx.array,
        key: mx.array,
        value: mx.array,
        scale: float | None = None,
        mask: mx.array | None = None,
    ) -> mx.array:
    
        dot_prods = query @ key.swapaxes(-2,-1) # (.., Lq, Lk)
        d_k = key.shape[-1]
        scale = scale if scale is not None else 1.0 / mx.sqrt(d_k)
        dot_prods *= scale
    
>       assert(mask.shape == dot_prods.shape)
               ^^^^^^^^^^
E       AttributeError: 'NoneType' object has no attribute 'shape'

src/tiny_llm/attention.py:26: AttributeError
________________ test_task_1_simple_attention[batch_0-f16-cpu] _________________

stream = DeviceType.cpu, precision = mlx.core.float16, batch_dimension = 0

    @pytest.mark.parametrize("stream", AVAILABLE_STREAMS, ids=AVAILABLE_STREAMS_IDS)
    @pytest.mark.parametrize("precision", PRECISIONS, ids=PRECISION_IDS)
    @pytest.mark.parametrize(
        "batch_dimension", [0, 1, 2], ids=["batch_0", "batch_1", "batch_2"]
    )
    def test_task_1_simple_attention(
        stream: mx.Stream, precision: mx.Dtype, batch_dimension: int
    ):
        """
        Test if `scaled_dot_product_attention_simple` can process Q/K/V correctly.
        We assume Q/K/V are of the same dimensions and test different batch dimensions.
        """
        with mx.stream(stream):
            if batch_dimension == 0:
                BATCH_SIZE = ()
            elif batch_dimension == 1:
                BATCH_SIZE = (2, 3)
            elif batch_dimension == 2:
                BATCH_SIZE = (2, 3, 3)
            DIM_L = 4
            DIM_D = 5
            for _ in range(100):
                query = mx.random.uniform(
                    shape=(*BATCH_SIZE, DIM_L, DIM_D), dtype=precision
                )
                key = mx.random.uniform(shape=(*BATCH_SIZE, DIM_L, DIM_D), dtype=precision)
                value = mx.random.uniform(
                    shape=(*BATCH_SIZE, DIM_L, DIM_D), dtype=precision
                )
                reference_output = mx.fast.scaled_dot_product_attention(
                    q=query.reshape(1, -1, DIM_L, DIM_D),
                    k=key.reshape(1, -1, DIM_L, DIM_D),
                    v=value.reshape(1, -1, DIM_L, DIM_D),
                    scale=1.0 / (DIM_D**0.5),
                ).reshape(*BATCH_SIZE, DIM_L, DIM_D)
>               user_output = scaled_dot_product_attention_simple(
                    query,
                    key,
                    value,
                )

tests/test_week_1_day_1.py:56: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

query = array([[0.283447, 0.284912, 0.95459, 0.702148, 0.847168],
       [0.254883, 0.627441, 0.572754, 0.266602, 0.989746],
 ...492, 0.549316, 0.98877, 0.551758, 0.841797],
       [0.235596, 0.169556, 0.159424, 0.810547, 0.416016]], dtype=float16)
key = array([[0.864746, 0.892578, 0.451416, 0.191528, 0.952637],
       [0.684082, 0.898926, 0.765625, 0.0524292, 0.818848],...682, 0.304932, 0.210571, 0.837402, 0.921875],
       [0.281738, 0.72168, 0.257568, 0.721191, 0.945801]], dtype=float16)
value = array([[0.243896, 0.871094, 0.765137, 0.981445, 0.252441],
       [0.214111, 0.165649, 0.0291901, 0.439941, 0.124573],...406, 0.0125122, 0.245239, 0.90918, 0.168945],
       [0.96875, 0.583496, 0.367676, 0.399902, 0.817871]], dtype=float16)
scale = array(0.447214, dtype=float32), mask = None

    def scaled_dot_product_attention_simple(
        query: mx.array,
        key: mx.array,
        value: mx.array,
        scale: float | None = None,
        mask: mx.array | None = None,
    ) -> mx.array:
    
        dot_prods = query @ key.swapaxes(-2,-1) # (.., Lq, Lk)
        d_k = key.shape[-1]
        scale = scale if scale is not None else 1.0 / mx.sqrt(d_k)
        dot_prods *= scale
    
>       assert(mask.shape == dot_prods.shape)
               ^^^^^^^^^^
E       AttributeError: 'NoneType' object has no attribute 'shape'

src/tiny_llm/attention.py:26: AttributeError
________________ test_task_1_simple_attention[batch_0-f16-gpu] _________________

stream = DeviceType.gpu, precision = mlx.core.float16, batch_dimension = 0

    @pytest.mark.parametrize("stream", AVAILABLE_STREAMS, ids=AVAILABLE_STREAMS_IDS)
    @pytest.mark.parametrize("precision", PRECISIONS, ids=PRECISION_IDS)
    @pytest.mark.parametrize(
        "batch_dimension", [0, 1, 2], ids=["batch_0", "batch_1", "batch_2"]
    )
    def test_task_1_simple_attention(
        stream: mx.Stream, precision: mx.Dtype, batch_dimension: int
    ):
        """
        Test if `scaled_dot_product_attention_simple` can process Q/K/V correctly.
        We assume Q/K/V are of the same dimensions and test different batch dimensions.
        """
        with mx.stream(stream):
            if batch_dimension == 0:
                BATCH_SIZE = ()
            elif batch_dimension == 1:
                BATCH_SIZE = (2, 3)
            elif batch_dimension == 2:
                BATCH_SIZE = (2, 3, 3)
            DIM_L = 4
            DIM_D = 5
            for _ in range(100):
                query = mx.random.uniform(
                    shape=(*BATCH_SIZE, DIM_L, DIM_D), dtype=precision
                )
                key = mx.random.uniform(shape=(*BATCH_SIZE, DIM_L, DIM_D), dtype=precision)
                value = mx.random.uniform(
                    shape=(*BATCH_SIZE, DIM_L, DIM_D), dtype=precision
                )
                reference_output = mx.fast.scaled_dot_product_attention(
                    q=query.reshape(1, -1, DIM_L, DIM_D),
                    k=key.reshape(1, -1, DIM_L, DIM_D),
                    v=value.reshape(1, -1, DIM_L, DIM_D),
                    scale=1.0 / (DIM_D**0.5),
                ).reshape(*BATCH_SIZE, DIM_L, DIM_D)
>               user_output = scaled_dot_product_attention_simple(
                    query,
                    key,
                    value,
                )

tests/test_week_1_day_1.py:56: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

query = array([[0.930664, 0.55957, 0.030899, 0.862793, 0.428955],
       [0.913086, 0.231201, 0.0479126, 0.600098, 0.391357],
..., 0.851074, 0.0906372, 0.0437317, 0.709961],
       [0.0658569, 0.686035, 0.959473, 0.681641, 0.93457]], dtype=float16)
key = array([[0.0326843, 0.274414, 0.280029, 0.632324, 0.772949],
       [0.96582, 0.106201, 0.315186, 0.411377, 0.314453],
...1, 0.852051, 0.00246239, 0.32373, 0.319824],
       [0.780762, 0.864258, 0.359131, 0.575195, 0.152344]], dtype=float16)
value = array([[0.705566, 0.261475, 0.19458, 0.462402, 0.392578],
       [0.468262, 0.543457, 0.0969238, 0.205811, 0.257568],
...06, 0.948242, 0.778809, 0.74707, 0.662109],
       [0.0202637, 0.478516, 0.665527, 0.217529, 0.362061]], dtype=float16)
scale = array(0.447214, dtype=float32), mask = None

    def scaled_dot_product_attention_simple(
        query: mx.array,
        key: mx.array,
        value: mx.array,
        scale: float | None = None,
        mask: mx.array | None = None,
    ) -> mx.array:
    
        dot_prods = query @ key.swapaxes(-2,-1) # (.., Lq, Lk)
        d_k = key.shape[-1]
        scale = scale if scale is not None else 1.0 / mx.sqrt(d_k)
        dot_prods *= scale
    
>       assert(mask.shape == dot_prods.shape)
               ^^^^^^^^^^
E       AttributeError: 'NoneType' object has no attribute 'shape'

src/tiny_llm/attention.py:26: AttributeError
________________ test_task_1_simple_attention[batch_1-f32-cpu] _________________

stream = DeviceType.cpu, precision = mlx.core.float32, batch_dimension = 1

    @pytest.mark.parametrize("stream", AVAILABLE_STREAMS, ids=AVAILABLE_STREAMS_IDS)
    @pytest.mark.parametrize("precision", PRECISIONS, ids=PRECISION_IDS)
    @pytest.mark.parametrize(
        "batch_dimension", [0, 1, 2], ids=["batch_0", "batch_1", "batch_2"]
    )
    def test_task_1_simple_attention(
        stream: mx.Stream, precision: mx.Dtype, batch_dimension: int
    ):
        """
        Test if `scaled_dot_product_attention_simple` can process Q/K/V correctly.
        We assume Q/K/V are of the same dimensions and test different batch dimensions.
        """
        with mx.stream(stream):
            if batch_dimension == 0:
                BATCH_SIZE = ()
            elif batch_dimension == 1:
                BATCH_SIZE = (2, 3)
            elif batch_dimension == 2:
                BATCH_SIZE = (2, 3, 3)
            DIM_L = 4
            DIM_D = 5
            for _ in range(100):
                query = mx.random.uniform(
                    shape=(*BATCH_SIZE, DIM_L, DIM_D), dtype=precision
                )
                key = mx.random.uniform(shape=(*BATCH_SIZE, DIM_L, DIM_D), dtype=precision)
                value = mx.random.uniform(
                    shape=(*BATCH_SIZE, DIM_L, DIM_D), dtype=precision
                )
                reference_output = mx.fast.scaled_dot_product_attention(
                    q=query.reshape(1, -1, DIM_L, DIM_D),
                    k=key.reshape(1, -1, DIM_L, DIM_D),
                    v=value.reshape(1, -1, DIM_L, DIM_D),
                    scale=1.0 / (DIM_D**0.5),
                ).reshape(*BATCH_SIZE, DIM_L, DIM_D)
>               user_output = scaled_dot_product_attention_simple(
                    query,
                    key,
                    value,
                )

tests/test_week_1_day_1.py:56: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

query = array([[[[0.821739, 0.825327, 0.777258, 0.931843, 0.238304],
         [0.793248, 0.171456, 0.501809, 0.616116, 0.38810...0.213533, 0.587932, 0.120073, 0.524255],
         [0.051239, 0.354561, 0.148153, 0.677942, 0.633297]]]], dtype=float32)
key = array([[[[0.0342811, 0.0690997, 0.948873, 0.472139, 0.379578],
         [0.160668, 0.580084, 0.507321, 0.103914, 0.528... 0.349794, 0.12916, 0.389652, 0.923577],
         [0.0545079, 0.96087, 0.997371, 0.593885, 0.531218]]]], dtype=float32)
value = array([[[[0.192501, 0.548995, 0.923039, 0.811886, 0.352312],
         [0.597191, 0.159079, 0.975936, 0.261959, 0.11222....31358, 0.541421, 0.299755, 0.818473],
         [0.0707171, 0.0700429, 0.935643, 0.829257, 0.626833]]]], dtype=float32)
scale = array(0.447214, dtype=float32), mask = None

    def scaled_dot_product_attention_simple(
        query: mx.array,
        key: mx.array,
        value: mx.array,
        scale: float | None = None,
        mask: mx.array | None = None,
    ) -> mx.array:
    
        dot_prods = query @ key.swapaxes(-2,-1) # (.., Lq, Lk)
        d_k = key.shape[-1]
        scale = scale if scale is not None else 1.0 / mx.sqrt(d_k)
        dot_prods *= scale
    
>       assert(mask.shape == dot_prods.shape)
               ^^^^^^^^^^
E       AttributeError: 'NoneType' object has no attribute 'shape'

src/tiny_llm/attention.py:26: AttributeError
________________ test_task_1_simple_attention[batch_1-f32-gpu] _________________

stream = DeviceType.gpu, precision = mlx.core.float32, batch_dimension = 1

    @pytest.mark.parametrize("stream", AVAILABLE_STREAMS, ids=AVAILABLE_STREAMS_IDS)
    @pytest.mark.parametrize("precision", PRECISIONS, ids=PRECISION_IDS)
    @pytest.mark.parametrize(
        "batch_dimension", [0, 1, 2], ids=["batch_0", "batch_1", "batch_2"]
    )
    def test_task_1_simple_attention(
        stream: mx.Stream, precision: mx.Dtype, batch_dimension: int
    ):
        """
        Test if `scaled_dot_product_attention_simple` can process Q/K/V correctly.
        We assume Q/K/V are of the same dimensions and test different batch dimensions.
        """
        with mx.stream(stream):
            if batch_dimension == 0:
                BATCH_SIZE = ()
            elif batch_dimension == 1:
                BATCH_SIZE = (2, 3)
            elif batch_dimension == 2:
                BATCH_SIZE = (2, 3, 3)
            DIM_L = 4
            DIM_D = 5
            for _ in range(100):
                query = mx.random.uniform(
                    shape=(*BATCH_SIZE, DIM_L, DIM_D), dtype=precision
                )
                key = mx.random.uniform(shape=(*BATCH_SIZE, DIM_L, DIM_D), dtype=precision)
                value = mx.random.uniform(
                    shape=(*BATCH_SIZE, DIM_L, DIM_D), dtype=precision
                )
                reference_output = mx.fast.scaled_dot_product_attention(
                    q=query.reshape(1, -1, DIM_L, DIM_D),
                    k=key.reshape(1, -1, DIM_L, DIM_D),
                    v=value.reshape(1, -1, DIM_L, DIM_D),
                    scale=1.0 / (DIM_D**0.5),
                ).reshape(*BATCH_SIZE, DIM_L, DIM_D)
>               user_output = scaled_dot_product_attention_simple(
                    query,
                    key,
                    value,
                )

tests/test_week_1_day_1.py:56: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

query = array([[[[0.0752083, 0.128673, 0.623027, 0.717857, 0.708355],
         [0.983996, 0.282729, 0.239828, 0.597557, 0.0430...0.904638, 0.888771, 0.351306, 0.308456],
         [0.545933, 0.784205, 0.103327, 0.398671, 0.293722]]]], dtype=float32)
key = array([[[[0.881608, 0.0805442, 0.990089, 0.716428, 0.929187],
         [0.622284, 0.0404385, 0.28732, 0.470074, 0.2618....263418, 0.87426, 0.847533, 0.0278586],
         [0.0207115, 0.343957, 0.692193, 0.225951, 0.761586]]]], dtype=float32)
value = array([[[[0.938537, 0.0442152, 0.987805, 0.39101, 0.037756],
         [0.721066, 0.835396, 0.466396, 0.193693, 0.95809... 0.955972, 0.681957, 0.391072, 0.65137],
         [0.291482, 0.490808, 0.569299, 0.928174, 0.643157]]]], dtype=float32)
scale = array(0.447214, dtype=float32), mask = None

    def scaled_dot_product_attention_simple(
        query: mx.array,
        key: mx.array,
        value: mx.array,
        scale: float | None = None,
        mask: mx.array | None = None,
    ) -> mx.array:
    
        dot_prods = query @ key.swapaxes(-2,-1) # (.., Lq, Lk)
        d_k = key.shape[-1]
        scale = scale if scale is not None else 1.0 / mx.sqrt(d_k)
        dot_prods *= scale
    
>       assert(mask.shape == dot_prods.shape)
               ^^^^^^^^^^
E       AttributeError: 'NoneType' object has no attribute 'shape'

src/tiny_llm/attention.py:26: AttributeError
________________ test_task_1_simple_attention[batch_1-f16-cpu] _________________

stream = DeviceType.cpu, precision = mlx.core.float16, batch_dimension = 1

    @pytest.mark.parametrize("stream", AVAILABLE_STREAMS, ids=AVAILABLE_STREAMS_IDS)
    @pytest.mark.parametrize("precision", PRECISIONS, ids=PRECISION_IDS)
    @pytest.mark.parametrize(
        "batch_dimension", [0, 1, 2], ids=["batch_0", "batch_1", "batch_2"]
    )
    def test_task_1_simple_attention(
        stream: mx.Stream, precision: mx.Dtype, batch_dimension: int
    ):
        """
        Test if `scaled_dot_product_attention_simple` can process Q/K/V correctly.
        We assume Q/K/V are of the same dimensions and test different batch dimensions.
        """
        with mx.stream(stream):
            if batch_dimension == 0:
                BATCH_SIZE = ()
            elif batch_dimension == 1:
                BATCH_SIZE = (2, 3)
            elif batch_dimension == 2:
                BATCH_SIZE = (2, 3, 3)
            DIM_L = 4
            DIM_D = 5
            for _ in range(100):
                query = mx.random.uniform(
                    shape=(*BATCH_SIZE, DIM_L, DIM_D), dtype=precision
                )
                key = mx.random.uniform(shape=(*BATCH_SIZE, DIM_L, DIM_D), dtype=precision)
                value = mx.random.uniform(
                    shape=(*BATCH_SIZE, DIM_L, DIM_D), dtype=precision
                )
                reference_output = mx.fast.scaled_dot_product_attention(
                    q=query.reshape(1, -1, DIM_L, DIM_D),
                    k=key.reshape(1, -1, DIM_L, DIM_D),
                    v=value.reshape(1, -1, DIM_L, DIM_D),
                    scale=1.0 / (DIM_D**0.5),
                ).reshape(*BATCH_SIZE, DIM_L, DIM_D)
>               user_output = scaled_dot_product_attention_simple(
                    query,
                    key,
                    value,
                )

tests/test_week_1_day_1.py:56: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

query = array([[[[0.241211, 0.445068, 0.705566, 0.997559, 0.770508],
         [0.855957, 0.205688, 0.188965, 0.31543, 0.076538... 0.193848, 0.984863, 0.960449, 0.864746],
         [0.52002, 0.805664, 0.438721, 0.757812, 0.467285]]]], dtype=float16)
key = array([[[[0.761719, 0.397949, 0.368896, 0.37085, 0.544434],
         [0.625977, 0.333008, 0.634766, 0.689941, 0.199951...752441, 0.256836, 0.996094, 0.689941],
         [0.0814819, 0.708984, 0.395508, 0.0950317, 0.414307]]]], dtype=float16)
value = array([[[[0.239746, 0.173462, 0.89502, 0.508789, 0.0783691],
         [0.708008, 0.24585, 0.138062, 0.288818, 0.796387... 0.937012, 0.29126, 0.509766, 0.363525],
         [0.647949, 0.130249, 0.0512085, 0.638184, 0.90332]]]], dtype=float16)
scale = array(0.447214, dtype=float32), mask = None

    def scaled_dot_product_attention_simple(
        query: mx.array,
        key: mx.array,
        value: mx.array,
        scale: float | None = None,
        mask: mx.array | None = None,
    ) -> mx.array:
    
        dot_prods = query @ key.swapaxes(-2,-1) # (.., Lq, Lk)
        d_k = key.shape[-1]
        scale = scale if scale is not None else 1.0 / mx.sqrt(d_k)
        dot_prods *= scale
    
>       assert(mask.shape == dot_prods.shape)
               ^^^^^^^^^^
E       AttributeError: 'NoneType' object has no attribute 'shape'

src/tiny_llm/attention.py:26: AttributeError
________________ test_task_1_simple_attention[batch_1-f16-gpu] _________________

stream = DeviceType.gpu, precision = mlx.core.float16, batch_dimension = 1

    @pytest.mark.parametrize("stream", AVAILABLE_STREAMS, ids=AVAILABLE_STREAMS_IDS)
    @pytest.mark.parametrize("precision", PRECISIONS, ids=PRECISION_IDS)
    @pytest.mark.parametrize(
        "batch_dimension", [0, 1, 2], ids=["batch_0", "batch_1", "batch_2"]
    )
    def test_task_1_simple_attention(
        stream: mx.Stream, precision: mx.Dtype, batch_dimension: int
    ):
        """
        Test if `scaled_dot_product_attention_simple` can process Q/K/V correctly.
        We assume Q/K/V are of the same dimensions and test different batch dimensions.
        """
        with mx.stream(stream):
            if batch_dimension == 0:
                BATCH_SIZE = ()
            elif batch_dimension == 1:
                BATCH_SIZE = (2, 3)
            elif batch_dimension == 2:
                BATCH_SIZE = (2, 3, 3)
            DIM_L = 4
            DIM_D = 5
            for _ in range(100):
                query = mx.random.uniform(
                    shape=(*BATCH_SIZE, DIM_L, DIM_D), dtype=precision
                )
                key = mx.random.uniform(shape=(*BATCH_SIZE, DIM_L, DIM_D), dtype=precision)
                value = mx.random.uniform(
                    shape=(*BATCH_SIZE, DIM_L, DIM_D), dtype=precision
                )
                reference_output = mx.fast.scaled_dot_product_attention(
                    q=query.reshape(1, -1, DIM_L, DIM_D),
                    k=key.reshape(1, -1, DIM_L, DIM_D),
                    v=value.reshape(1, -1, DIM_L, DIM_D),
                    scale=1.0 / (DIM_D**0.5),
                ).reshape(*BATCH_SIZE, DIM_L, DIM_D)
>               user_output = scaled_dot_product_attention_simple(
                    query,
                    key,
                    value,
                )

tests/test_week_1_day_1.py:56: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

query = array([[[[0.452881, 0.268066, 0.692383, 0.975098, 0.378906],
         [0.921875, 0.109863, 0.901855, 0.650879, 0.53271....777832, 0.952637, 0.106323, 0.496826],
         [0.355957, 0.500977, 0.340088, 0.0411987, 0.167969]]]], dtype=float16)
key = array([[[[0.993164, 0.923828, 0.00741959, 0.269775, 0.0386658],
         [0.219971, 0.100525, 0.873535, 0.930664, 0.11...4, 0.960938, 0.567871, 0.33252, 0.655273],
         [0.103821, 0.6875, 0.490234, 0.706055, 0.360107]]]], dtype=float16)
value = array([[[[0.931641, 0.0629883, 0.792969, 0.390625, 0.501465],
         [0.920898, 0.0328064, 0.865723, 0.161377, 0.758... 0.172241, 0.886719, 0.320068, 0.751465],
         [0.0154572, 0.659668, 0.42749, 0.20813, 0.406738]]]], dtype=float16)
scale = array(0.447214, dtype=float32), mask = None

    def scaled_dot_product_attention_simple(
        query: mx.array,
        key: mx.array,
        value: mx.array,
        scale: float | None = None,
        mask: mx.array | None = None,
    ) -> mx.array:
    
        dot_prods = query @ key.swapaxes(-2,-1) # (.., Lq, Lk)
        d_k = key.shape[-1]
        scale = scale if scale is not None else 1.0 / mx.sqrt(d_k)
        dot_prods *= scale
    
>       assert(mask.shape == dot_prods.shape)
               ^^^^^^^^^^
E       AttributeError: 'NoneType' object has no attribute 'shape'

src/tiny_llm/attention.py:26: AttributeError
________________ test_task_1_simple_attention[batch_2-f32-cpu] _________________

stream = DeviceType.cpu, precision = mlx.core.float32, batch_dimension = 2

    @pytest.mark.parametrize("stream", AVAILABLE_STREAMS, ids=AVAILABLE_STREAMS_IDS)
    @pytest.mark.parametrize("precision", PRECISIONS, ids=PRECISION_IDS)
    @pytest.mark.parametrize(
        "batch_dimension", [0, 1, 2], ids=["batch_0", "batch_1", "batch_2"]
    )
    def test_task_1_simple_attention(
        stream: mx.Stream, precision: mx.Dtype, batch_dimension: int
    ):
        """
        Test if `scaled_dot_product_attention_simple` can process Q/K/V correctly.
        We assume Q/K/V are of the same dimensions and test different batch dimensions.
        """
        with mx.stream(stream):
            if batch_dimension == 0:
                BATCH_SIZE = ()
            elif batch_dimension == 1:
                BATCH_SIZE = (2, 3)
            elif batch_dimension == 2:
                BATCH_SIZE = (2, 3, 3)
            DIM_L = 4
            DIM_D = 5
            for _ in range(100):
                query = mx.random.uniform(
                    shape=(*BATCH_SIZE, DIM_L, DIM_D), dtype=precision
                )
                key = mx.random.uniform(shape=(*BATCH_SIZE, DIM_L, DIM_D), dtype=precision)
                value = mx.random.uniform(
                    shape=(*BATCH_SIZE, DIM_L, DIM_D), dtype=precision
                )
                reference_output = mx.fast.scaled_dot_product_attention(
                    q=query.reshape(1, -1, DIM_L, DIM_D),
                    k=key.reshape(1, -1, DIM_L, DIM_D),
                    v=value.reshape(1, -1, DIM_L, DIM_D),
                    scale=1.0 / (DIM_D**0.5),
                ).reshape(*BATCH_SIZE, DIM_L, DIM_D)
>               user_output = scaled_dot_product_attention_simple(
                    query,
                    key,
                    value,
                )

tests/test_week_1_day_1.py:56: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

query = array([[[[[0.434797, 0.311319, 0.278833, 0.734156, 0.0304326],
          [0.151027, 0.490201, 0.661192, 0.522207, 0.52...35864, 0.717387, 0.580159, 0.609743],
          [0.149139, 0.0688858, 0.944623, 0.309373, 0.822779]]]]], dtype=float32)
key = array([[[[[0.714927, 0.875742, 0.59897, 0.764912, 0.886652],
          [0.29383, 0.637227, 0.694234, 0.180738, 0.35061...300883, 0.0687425, 0.697613, 0.993408],
          [0.407876, 0.736222, 0.29862, 0.774487, 0.548716]]]]], dtype=float32)
value = array([[[[[0.421315, 0.324133, 0.0645986, 0.036713, 0.0513608],
          [0.00319357, 0.653145, 0.522018, 0.118682, 0...76, 0.0140503, 0.14448, 0.0684592],
          [0.708145, 0.0712617, 0.00884521, 0.889444, 0.543247]]]]], dtype=float32)
scale = array(0.447214, dtype=float32), mask = None

    def scaled_dot_product_attention_simple(
        query: mx.array,
        key: mx.array,
        value: mx.array,
        scale: float | None = None,
        mask: mx.array | None = None,
    ) -> mx.array:
    
        dot_prods = query @ key.swapaxes(-2,-1) # (.., Lq, Lk)
        d_k = key.shape[-1]
        scale = scale if scale is not None else 1.0 / mx.sqrt(d_k)
        dot_prods *= scale
    
>       assert(mask.shape == dot_prods.shape)
               ^^^^^^^^^^
E       AttributeError: 'NoneType' object has no attribute 'shape'

src/tiny_llm/attention.py:26: AttributeError
________________ test_task_1_simple_attention[batch_2-f32-gpu] _________________

stream = DeviceType.gpu, precision = mlx.core.float32, batch_dimension = 2

    @pytest.mark.parametrize("stream", AVAILABLE_STREAMS, ids=AVAILABLE_STREAMS_IDS)
    @pytest.mark.parametrize("precision", PRECISIONS, ids=PRECISION_IDS)
    @pytest.mark.parametrize(
        "batch_dimension", [0, 1, 2], ids=["batch_0", "batch_1", "batch_2"]
    )
    def test_task_1_simple_attention(
        stream: mx.Stream, precision: mx.Dtype, batch_dimension: int
    ):
        """
        Test if `scaled_dot_product_attention_simple` can process Q/K/V correctly.
        We assume Q/K/V are of the same dimensions and test different batch dimensions.
        """
        with mx.stream(stream):
            if batch_dimension == 0:
                BATCH_SIZE = ()
            elif batch_dimension == 1:
                BATCH_SIZE = (2, 3)
            elif batch_dimension == 2:
                BATCH_SIZE = (2, 3, 3)
            DIM_L = 4
            DIM_D = 5
            for _ in range(100):
                query = mx.random.uniform(
                    shape=(*BATCH_SIZE, DIM_L, DIM_D), dtype=precision
                )
                key = mx.random.uniform(shape=(*BATCH_SIZE, DIM_L, DIM_D), dtype=precision)
                value = mx.random.uniform(
                    shape=(*BATCH_SIZE, DIM_L, DIM_D), dtype=precision
                )
                reference_output = mx.fast.scaled_dot_product_attention(
                    q=query.reshape(1, -1, DIM_L, DIM_D),
                    k=key.reshape(1, -1, DIM_L, DIM_D),
                    v=value.reshape(1, -1, DIM_L, DIM_D),
                    scale=1.0 / (DIM_D**0.5),
                ).reshape(*BATCH_SIZE, DIM_L, DIM_D)
>               user_output = scaled_dot_product_attention_simple(
                    query,
                    key,
                    value,
                )

tests/test_week_1_day_1.py:56: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

query = array([[[[[0.329624, 0.866652, 0.844936, 0.677033, 0.968275],
          [0.780678, 0.400522, 0.285754, 0.175599, 0.945...947388, 0.546843, 0.817715, 0.462723],
          [0.214926, 0.541264, 0.583367, 0.962862, 0.605909]]]]], dtype=float32)
key = array([[[[[0.379086, 0.427273, 0.894482, 0.0182877, 0.538872],
          [0.861824, 0.900339, 0.52598, 0.897833, 0.201....528117, 0.751802, 0.666463, 0.9633],
          [0.230386, 0.0357791, 0.901768, 0.780662, 0.564296]]]]], dtype=float32)
value = array([[[[[0.507958, 0.222341, 0.814997, 0.30072, 0.245794],
          [0.216864, 0.886554, 0.533648, 0.941091, 0.0495...143, 0.760586, 0.0995805, 0.679647],
          [0.942151, 0.700891, 0.856341, 0.0370862, 0.0200991]]]]], dtype=float32)
scale = array(0.447214, dtype=float32), mask = None

    def scaled_dot_product_attention_simple(
        query: mx.array,
        key: mx.array,
        value: mx.array,
        scale: float | None = None,
        mask: mx.array | None = None,
    ) -> mx.array:
    
        dot_prods = query @ key.swapaxes(-2,-1) # (.., Lq, Lk)
        d_k = key.shape[-1]
        scale = scale if scale is not None else 1.0 / mx.sqrt(d_k)
        dot_prods *= scale
    
>       assert(mask.shape == dot_prods.shape)
               ^^^^^^^^^^
E       AttributeError: 'NoneType' object has no attribute 'shape'

src/tiny_llm/attention.py:26: AttributeError
________________ test_task_1_simple_attention[batch_2-f16-cpu] _________________

stream = DeviceType.cpu, precision = mlx.core.float16, batch_dimension = 2

    @pytest.mark.parametrize("stream", AVAILABLE_STREAMS, ids=AVAILABLE_STREAMS_IDS)
    @pytest.mark.parametrize("precision", PRECISIONS, ids=PRECISION_IDS)
    @pytest.mark.parametrize(
        "batch_dimension", [0, 1, 2], ids=["batch_0", "batch_1", "batch_2"]
    )
    def test_task_1_simple_attention(
        stream: mx.Stream, precision: mx.Dtype, batch_dimension: int
    ):
        """
        Test if `scaled_dot_product_attention_simple` can process Q/K/V correctly.
        We assume Q/K/V are of the same dimensions and test different batch dimensions.
        """
        with mx.stream(stream):
            if batch_dimension == 0:
                BATCH_SIZE = ()
            elif batch_dimension == 1:
                BATCH_SIZE = (2, 3)
            elif batch_dimension == 2:
                BATCH_SIZE = (2, 3, 3)
            DIM_L = 4
            DIM_D = 5
            for _ in range(100):
                query = mx.random.uniform(
                    shape=(*BATCH_SIZE, DIM_L, DIM_D), dtype=precision
                )
                key = mx.random.uniform(shape=(*BATCH_SIZE, DIM_L, DIM_D), dtype=precision)
                value = mx.random.uniform(
                    shape=(*BATCH_SIZE, DIM_L, DIM_D), dtype=precision
                )
                reference_output = mx.fast.scaled_dot_product_attention(
                    q=query.reshape(1, -1, DIM_L, DIM_D),
                    k=key.reshape(1, -1, DIM_L, DIM_D),
                    v=value.reshape(1, -1, DIM_L, DIM_D),
                    scale=1.0 / (DIM_D**0.5),
                ).reshape(*BATCH_SIZE, DIM_L, DIM_D)
>               user_output = scaled_dot_product_attention_simple(
                    query,
                    key,
                    value,
                )

tests/test_week_1_day_1.py:56: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

query = array([[[[[0.1427, 0.917969, 0.847656, 0.368652, 0.793457],
          [0.621582, 0.796387, 0.247437, 0.203491, 0.91503... 0.87793, 0.310547, 0.90625, 0.76709],
          [0.0778809, 0.580566, 0.835938, 0.22998, 0.337646]]]]], dtype=float16)
key = array([[[[[0.588867, 0.701172, 0.798828, 0.778809, 0.0651855],
          [0.0628662, 0.668945, 0.299805, 0.835449, 0.1...623047, 0.474121, 0.399658, 0.516602],
          [0.82959, 0.398682, 0.860352, 0.0695801, 0.462158]]]]], dtype=float16)
value = array([[[[[0.671875, 0.713379, 0.941895, 0.73291, 0.326172],
          [0.567871, 0.540039, 0.414062, 0.541016, 0.3833...510742, 0.390869, 0.432129, 0.499268],
          [0.87793, 0.0916748, 0.732422, 0.669922, 0.583984]]]]], dtype=float16)
scale = array(0.447214, dtype=float32), mask = None

    def scaled_dot_product_attention_simple(
        query: mx.array,
        key: mx.array,
        value: mx.array,
        scale: float | None = None,
        mask: mx.array | None = None,
    ) -> mx.array:
    
        dot_prods = query @ key.swapaxes(-2,-1) # (.., Lq, Lk)
        d_k = key.shape[-1]
        scale = scale if scale is not None else 1.0 / mx.sqrt(d_k)
        dot_prods *= scale
    
>       assert(mask.shape == dot_prods.shape)
               ^^^^^^^^^^
E       AttributeError: 'NoneType' object has no attribute 'shape'

src/tiny_llm/attention.py:26: AttributeError
________________ test_task_1_simple_attention[batch_2-f16-gpu] _________________

stream = DeviceType.gpu, precision = mlx.core.float16, batch_dimension = 2

    @pytest.mark.parametrize("stream", AVAILABLE_STREAMS, ids=AVAILABLE_STREAMS_IDS)
    @pytest.mark.parametrize("precision", PRECISIONS, ids=PRECISION_IDS)
    @pytest.mark.parametrize(
        "batch_dimension", [0, 1, 2], ids=["batch_0", "batch_1", "batch_2"]
    )
    def test_task_1_simple_attention(
        stream: mx.Stream, precision: mx.Dtype, batch_dimension: int
    ):
        """
        Test if `scaled_dot_product_attention_simple` can process Q/K/V correctly.
        We assume Q/K/V are of the same dimensions and test different batch dimensions.
        """
        with mx.stream(stream):
            if batch_dimension == 0:
                BATCH_SIZE = ()
            elif batch_dimension == 1:
                BATCH_SIZE = (2, 3)
            elif batch_dimension == 2:
                BATCH_SIZE = (2, 3, 3)
            DIM_L = 4
            DIM_D = 5
            for _ in range(100):
                query = mx.random.uniform(
                    shape=(*BATCH_SIZE, DIM_L, DIM_D), dtype=precision
                )
                key = mx.random.uniform(shape=(*BATCH_SIZE, DIM_L, DIM_D), dtype=precision)
                value = mx.random.uniform(
                    shape=(*BATCH_SIZE, DIM_L, DIM_D), dtype=precision
                )
                reference_output = mx.fast.scaled_dot_product_attention(
                    q=query.reshape(1, -1, DIM_L, DIM_D),
                    k=key.reshape(1, -1, DIM_L, DIM_D),
                    v=value.reshape(1, -1, DIM_L, DIM_D),
                    scale=1.0 / (DIM_D**0.5),
                ).reshape(*BATCH_SIZE, DIM_L, DIM_D)
>               user_output = scaled_dot_product_attention_simple(
                    query,
                    key,
                    value,
                )

tests/test_week_1_day_1.py:56: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

query = array([[[[[0.724609, 0.0378723, 0.470459, 0.613281, 0.432617],
          [0.0905151, 0.748535, 0.306152, 0.834961, 0.4... 0.725098, 0.519531, 0.839355, 0.5],
          [0.419434, 0.911133, 0.918945, 0.0518494, 0.0123444]]]]], dtype=float16)
key = array([[[[[0.022583, 0.293701, 0.0504761, 0.252441, 0.619629],
          [0.883789, 0.932617, 0.37793, 0.0366821, 0.82...6416, 0.466553, 0.213501, 0.348633],
          [0.433594, 0.969238, 0.0474548, 0.982422, 0.0173645]]]]], dtype=float16)
value = array([[[[[0.183472, 0.682129, 0.224854, 0.641113, 0.435303],
          [0.539062, 0.252441, 0.511719, 0.567871, 0.413...817383, 0.996094, 0.757324, 0.465332],
          [0.82666, 0.749512, 0.167236, 0.359375, 0.0828247]]]]], dtype=float16)
scale = array(0.447214, dtype=float32), mask = None

    def scaled_dot_product_attention_simple(
        query: mx.array,
        key: mx.array,
        value: mx.array,
        scale: float | None = None,
        mask: mx.array | None = None,
    ) -> mx.array:
    
        dot_prods = query @ key.swapaxes(-2,-1) # (.., Lq, Lk)
        d_k = key.shape[-1]
        scale = scale if scale is not None else 1.0 / mx.sqrt(d_k)
        dot_prods *= scale
    
>       assert(mask.shape == dot_prods.shape)
               ^^^^^^^^^^
E       AttributeError: 'NoneType' object has no attribute 'shape'

src/tiny_llm/attention.py:26: AttributeError
___________ test_task_1_simple_attention_scale_mask[batch_0-f32-cpu] ___________

stream = DeviceType.cpu, precision = mlx.core.float32, batch_dimension = 0

    @pytest.mark.parametrize("stream", AVAILABLE_STREAMS, ids=AVAILABLE_STREAMS_IDS)
    @pytest.mark.parametrize("precision", PRECISIONS, ids=PRECISION_IDS)
    @pytest.mark.parametrize(
        "batch_dimension", [0, 1, 2], ids=["batch_0", "batch_1", "batch_2"]
    )
    def test_task_1_simple_attention_scale_mask(
        stream: mx.Stream, precision: mx.Dtype, batch_dimension: int
    ):
        """
        Test if `scaled_dot_product_attention_simple` can process scale and mask correctly.
        """
        with mx.stream(stream):
            if batch_dimension == 0:
                BATCH_SIZE = ()
            elif batch_dimension == 1:
                BATCH_SIZE = (2, 3)
            elif batch_dimension == 2:
                BATCH_SIZE = (2, 3, 3)
            DIM_L = 4
            DIM_D = 5
            for _ in range(100):
                query = mx.random.uniform(
                    shape=(*BATCH_SIZE, DIM_L, DIM_D), dtype=precision
                )
                key = mx.random.uniform(shape=(*BATCH_SIZE, DIM_L, DIM_D), dtype=precision)
                value = mx.random.uniform(
                    shape=(*BATCH_SIZE, DIM_L, DIM_D), dtype=precision
                )
                mask = mx.random.uniform(shape=(*BATCH_SIZE, DIM_L, DIM_L), dtype=precision)
                scale = 0.5
                reference_output = mx.fast.scaled_dot_product_attention(
                    q=query.reshape(1, -1, DIM_L, DIM_D),
                    k=key.reshape(1, -1, DIM_L, DIM_D),
                    v=value.reshape(1, -1, DIM_L, DIM_D),
                    scale=scale,
                    mask=mask.reshape(1, -1, DIM_L, DIM_L),
                ).reshape(*BATCH_SIZE, DIM_L, DIM_D)
                user_output = scaled_dot_product_attention_simple(
                    query,
                    key,
                    value,
                    scale=scale,
                    mask=mask,
                )
>               assert_allclose(user_output, reference_output, precision=precision)

tests/test_week_1_day_1.py:108: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

a = array([[nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan]], dtype=float32)
b = array([[0.48541853, 0.67503047, 0.52267563, 0.42281717, 0.33645743],
       [0.5011096 , 0.633466  , 0.5642497 , 0.403...  , 0.42319188, 0.37654704],
       [0.4774993 , 0.73906815, 0.46477115, 0.29169542, 0.34591845]],
      dtype=float32)
precision = mlx.core.float32, rtol = 1e-05, atol = 1e-08, message = None

    def assert_allclose(
        a: mx.array,
        b: mx.array,
        precision: mx.Dtype,
        rtol: float | None = None,
        atol: float | None = None,
        message: str | None = None,
    ):
        a = np.array(a)
        b = np.array(b)
        if precision == mx.float32:
            rtol = rtol or 1.0e-5
            atol = atol or 1.0e-8
        elif precision == mx.float16:
            rtol = rtol or 3.0e-2
            atol = atol or 1.0e-5
        else:
            raise ValueError(f"Unsupported precision: {precision}")
        assert a.shape == b.shape, f"shape mismatch: {a.shape} vs {b.shape}"
        if not np.allclose(a, b, rtol=rtol, atol=atol):
            diff = np.invert(np.isclose(a, b, rtol=rtol, atol=atol))
            if diff.size > 10000 and np.sum(diff) <= 3:
                # if only a small number of elements are different in a large array, probably fine
                return
            with np.printoptions(precision=3, suppress=True):
                print("a=", a)
                print("b=", b)
                print("diff_a=", a * diff)
                print("diff_b=", b * diff)
                print("diff_a_val=", a[diff])
                print("diff_b_val=", b[diff])
>               assert False, f"result mismatch: {message}"
                       ^^^^^
E               AssertionError: result mismatch: None

tests/utils.py:42: AssertionError
----------------------------- Captured stdout call -----------------------------
a= [[nan nan nan nan nan]
 [nan nan nan nan nan]
 [nan nan nan nan nan]
 [nan nan nan nan nan]]
b= [[0.485 0.675 0.523 0.423 0.336]
 [0.501 0.633 0.564 0.404 0.366]
 [0.571 0.696 0.541 0.423 0.377]
 [0.477 0.739 0.465 0.292 0.346]]
diff_a= [[nan nan nan nan nan]
 [nan nan nan nan nan]
 [nan nan nan nan nan]
 [nan nan nan nan nan]]
diff_b= [[0.485 0.675 0.523 0.423 0.336]
 [0.501 0.633 0.564 0.404 0.366]
 [0.571 0.696 0.541 0.423 0.377]
 [0.477 0.739 0.465 0.292 0.346]]
diff_a_val= [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan
 nan nan]
diff_b_val= [0.485 0.675 0.523 0.423 0.336 0.501 0.633 0.564 0.404 0.366 0.571 0.696
 0.541 0.423 0.377 0.477 0.739 0.465 0.292 0.346]
___________ test_task_1_simple_attention_scale_mask[batch_0-f32-gpu] ___________

stream = DeviceType.gpu, precision = mlx.core.float32, batch_dimension = 0

    @pytest.mark.parametrize("stream", AVAILABLE_STREAMS, ids=AVAILABLE_STREAMS_IDS)
    @pytest.mark.parametrize("precision", PRECISIONS, ids=PRECISION_IDS)
    @pytest.mark.parametrize(
        "batch_dimension", [0, 1, 2], ids=["batch_0", "batch_1", "batch_2"]
    )
    def test_task_1_simple_attention_scale_mask(
        stream: mx.Stream, precision: mx.Dtype, batch_dimension: int
    ):
        """
        Test if `scaled_dot_product_attention_simple` can process scale and mask correctly.
        """
        with mx.stream(stream):
            if batch_dimension == 0:
                BATCH_SIZE = ()
            elif batch_dimension == 1:
                BATCH_SIZE = (2, 3)
            elif batch_dimension == 2:
                BATCH_SIZE = (2, 3, 3)
            DIM_L = 4
            DIM_D = 5
            for _ in range(100):
                query = mx.random.uniform(
                    shape=(*BATCH_SIZE, DIM_L, DIM_D), dtype=precision
                )
                key = mx.random.uniform(shape=(*BATCH_SIZE, DIM_L, DIM_D), dtype=precision)
                value = mx.random.uniform(
                    shape=(*BATCH_SIZE, DIM_L, DIM_D), dtype=precision
                )
                mask = mx.random.uniform(shape=(*BATCH_SIZE, DIM_L, DIM_L), dtype=precision)
                scale = 0.5
                reference_output = mx.fast.scaled_dot_product_attention(
                    q=query.reshape(1, -1, DIM_L, DIM_D),
                    k=key.reshape(1, -1, DIM_L, DIM_D),
                    v=value.reshape(1, -1, DIM_L, DIM_D),
                    scale=scale,
                    mask=mask.reshape(1, -1, DIM_L, DIM_L),
                ).reshape(*BATCH_SIZE, DIM_L, DIM_D)
                user_output = scaled_dot_product_attention_simple(
                    query,
                    key,
                    value,
                    scale=scale,
                    mask=mask,
                )
>               assert_allclose(user_output, reference_output, precision=precision)

tests/test_week_1_day_1.py:108: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

a = array([[nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan]], dtype=float32)
b = array([[0.6376929 , 0.5440656 , 0.68385327, 0.33630127, 0.4603221 ],
       [0.68141174, 0.6319761 , 0.6735235 , 0.437...3 , 0.2996334 , 0.3914179 ],
       [0.6281885 , 0.5450311 , 0.63637304, 0.4183253 , 0.4721094 ]],
      dtype=float32)
precision = mlx.core.float32, rtol = 1e-05, atol = 1e-08, message = None

    def assert_allclose(
        a: mx.array,
        b: mx.array,
        precision: mx.Dtype,
        rtol: float | None = None,
        atol: float | None = None,
        message: str | None = None,
    ):
        a = np.array(a)
        b = np.array(b)
        if precision == mx.float32:
            rtol = rtol or 1.0e-5
            atol = atol or 1.0e-8
        elif precision == mx.float16:
            rtol = rtol or 3.0e-2
            atol = atol or 1.0e-5
        else:
            raise ValueError(f"Unsupported precision: {precision}")
        assert a.shape == b.shape, f"shape mismatch: {a.shape} vs {b.shape}"
        if not np.allclose(a, b, rtol=rtol, atol=atol):
            diff = np.invert(np.isclose(a, b, rtol=rtol, atol=atol))
            if diff.size > 10000 and np.sum(diff) <= 3:
                # if only a small number of elements are different in a large array, probably fine
                return
            with np.printoptions(precision=3, suppress=True):
                print("a=", a)
                print("b=", b)
                print("diff_a=", a * diff)
                print("diff_b=", b * diff)
                print("diff_a_val=", a[diff])
                print("diff_b_val=", b[diff])
>               assert False, f"result mismatch: {message}"
                       ^^^^^
E               AssertionError: result mismatch: None

tests/utils.py:42: AssertionError
----------------------------- Captured stdout call -----------------------------
a= [[nan nan nan nan nan]
 [nan nan nan nan nan]
 [nan nan nan nan nan]
 [nan nan nan nan nan]]
b= [[0.638 0.544 0.684 0.336 0.46 ]
 [0.681 0.632 0.674 0.438 0.483]
 [0.596 0.588 0.637 0.3   0.391]
 [0.628 0.545 0.636 0.418 0.472]]
diff_a= [[nan nan nan nan nan]
 [nan nan nan nan nan]
 [nan nan nan nan nan]
 [nan nan nan nan nan]]
diff_b= [[0.638 0.544 0.684 0.336 0.46 ]
 [0.681 0.632 0.674 0.438 0.483]
 [0.596 0.588 0.637 0.3   0.391]
 [0.628 0.545 0.636 0.418 0.472]]
diff_a_val= [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan
 nan nan]
diff_b_val= [0.638 0.544 0.684 0.336 0.46  0.681 0.632 0.674 0.438 0.483 0.596 0.588
 0.637 0.3   0.391 0.628 0.545 0.636 0.418 0.472]
___________ test_task_1_simple_attention_scale_mask[batch_0-f16-cpu] ___________

stream = DeviceType.cpu, precision = mlx.core.float16, batch_dimension = 0

    @pytest.mark.parametrize("stream", AVAILABLE_STREAMS, ids=AVAILABLE_STREAMS_IDS)
    @pytest.mark.parametrize("precision", PRECISIONS, ids=PRECISION_IDS)
    @pytest.mark.parametrize(
        "batch_dimension", [0, 1, 2], ids=["batch_0", "batch_1", "batch_2"]
    )
    def test_task_1_simple_attention_scale_mask(
        stream: mx.Stream, precision: mx.Dtype, batch_dimension: int
    ):
        """
        Test if `scaled_dot_product_attention_simple` can process scale and mask correctly.
        """
        with mx.stream(stream):
            if batch_dimension == 0:
                BATCH_SIZE = ()
            elif batch_dimension == 1:
                BATCH_SIZE = (2, 3)
            elif batch_dimension == 2:
                BATCH_SIZE = (2, 3, 3)
            DIM_L = 4
            DIM_D = 5
            for _ in range(100):
                query = mx.random.uniform(
                    shape=(*BATCH_SIZE, DIM_L, DIM_D), dtype=precision
                )
                key = mx.random.uniform(shape=(*BATCH_SIZE, DIM_L, DIM_D), dtype=precision)
                value = mx.random.uniform(
                    shape=(*BATCH_SIZE, DIM_L, DIM_D), dtype=precision
                )
                mask = mx.random.uniform(shape=(*BATCH_SIZE, DIM_L, DIM_L), dtype=precision)
                scale = 0.5
                reference_output = mx.fast.scaled_dot_product_attention(
                    q=query.reshape(1, -1, DIM_L, DIM_D),
                    k=key.reshape(1, -1, DIM_L, DIM_D),
                    v=value.reshape(1, -1, DIM_L, DIM_D),
                    scale=scale,
                    mask=mask.reshape(1, -1, DIM_L, DIM_L),
                ).reshape(*BATCH_SIZE, DIM_L, DIM_D)
                user_output = scaled_dot_product_attention_simple(
                    query,
                    key,
                    value,
                    scale=scale,
                    mask=mask,
                )
>               assert_allclose(user_output, reference_output, precision=precision)

tests/test_week_1_day_1.py:108: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

a = array([[nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan]], dtype=float32)
b = array([[0.3853, 0.5703, 0.4182, 0.452 , 0.5913],
       [0.2969, 0.687 , 0.4624, 0.5454, 0.528 ],
       [0.408 , 0.6514, 0.516 , 0.5137, 0.513 ],
       [0.4106, 0.667 , 0.526 , 0.507 , 0.5127]], dtype=float16)
precision = mlx.core.float16, rtol = 0.03, atol = 1e-05, message = None

    def assert_allclose(
        a: mx.array,
        b: mx.array,
        precision: mx.Dtype,
        rtol: float | None = None,
        atol: float | None = None,
        message: str | None = None,
    ):
        a = np.array(a)
        b = np.array(b)
        if precision == mx.float32:
            rtol = rtol or 1.0e-5
            atol = atol or 1.0e-8
        elif precision == mx.float16:
            rtol = rtol or 3.0e-2
            atol = atol or 1.0e-5
        else:
            raise ValueError(f"Unsupported precision: {precision}")
        assert a.shape == b.shape, f"shape mismatch: {a.shape} vs {b.shape}"
        if not np.allclose(a, b, rtol=rtol, atol=atol):
            diff = np.invert(np.isclose(a, b, rtol=rtol, atol=atol))
            if diff.size > 10000 and np.sum(diff) <= 3:
                # if only a small number of elements are different in a large array, probably fine
                return
            with np.printoptions(precision=3, suppress=True):
                print("a=", a)
                print("b=", b)
                print("diff_a=", a * diff)
                print("diff_b=", b * diff)
                print("diff_a_val=", a[diff])
                print("diff_b_val=", b[diff])
>               assert False, f"result mismatch: {message}"
                       ^^^^^
E               AssertionError: result mismatch: None

tests/utils.py:42: AssertionError
----------------------------- Captured stdout call -----------------------------
a= [[nan nan nan nan nan]
 [nan nan nan nan nan]
 [nan nan nan nan nan]
 [nan nan nan nan nan]]
b= [[0.385 0.57  0.418 0.452 0.591]
 [0.297 0.687 0.462 0.545 0.528]
 [0.408 0.651 0.516 0.514 0.513]
 [0.411 0.667 0.526 0.507 0.513]]
diff_a= [[nan nan nan nan nan]
 [nan nan nan nan nan]
 [nan nan nan nan nan]
 [nan nan nan nan nan]]
diff_b= [[0.385 0.57  0.418 0.452 0.591]
 [0.297 0.687 0.462 0.545 0.528]
 [0.408 0.651 0.516 0.514 0.513]
 [0.411 0.667 0.526 0.507 0.513]]
diff_a_val= [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan
 nan nan]
diff_b_val= [0.385 0.57  0.418 0.452 0.591 0.297 0.687 0.462 0.545 0.528 0.408 0.651
 0.516 0.514 0.513 0.411 0.667 0.526 0.507 0.513]
___________ test_task_1_simple_attention_scale_mask[batch_0-f16-gpu] ___________

stream = DeviceType.gpu, precision = mlx.core.float16, batch_dimension = 0

    @pytest.mark.parametrize("stream", AVAILABLE_STREAMS, ids=AVAILABLE_STREAMS_IDS)
    @pytest.mark.parametrize("precision", PRECISIONS, ids=PRECISION_IDS)
    @pytest.mark.parametrize(
        "batch_dimension", [0, 1, 2], ids=["batch_0", "batch_1", "batch_2"]
    )
    def test_task_1_simple_attention_scale_mask(
        stream: mx.Stream, precision: mx.Dtype, batch_dimension: int
    ):
        """
        Test if `scaled_dot_product_attention_simple` can process scale and mask correctly.
        """
        with mx.stream(stream):
            if batch_dimension == 0:
                BATCH_SIZE = ()
            elif batch_dimension == 1:
                BATCH_SIZE = (2, 3)
            elif batch_dimension == 2:
                BATCH_SIZE = (2, 3, 3)
            DIM_L = 4
            DIM_D = 5
            for _ in range(100):
                query = mx.random.uniform(
                    shape=(*BATCH_SIZE, DIM_L, DIM_D), dtype=precision
                )
                key = mx.random.uniform(shape=(*BATCH_SIZE, DIM_L, DIM_D), dtype=precision)
                value = mx.random.uniform(
                    shape=(*BATCH_SIZE, DIM_L, DIM_D), dtype=precision
                )
                mask = mx.random.uniform(shape=(*BATCH_SIZE, DIM_L, DIM_L), dtype=precision)
                scale = 0.5
                reference_output = mx.fast.scaled_dot_product_attention(
                    q=query.reshape(1, -1, DIM_L, DIM_D),
                    k=key.reshape(1, -1, DIM_L, DIM_D),
                    v=value.reshape(1, -1, DIM_L, DIM_D),
                    scale=scale,
                    mask=mask.reshape(1, -1, DIM_L, DIM_L),
                ).reshape(*BATCH_SIZE, DIM_L, DIM_D)
                user_output = scaled_dot_product_attention_simple(
                    query,
                    key,
                    value,
                    scale=scale,
                    mask=mask,
                )
>               assert_allclose(user_output, reference_output, precision=precision)

tests/test_week_1_day_1.py:108: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

a = array([[nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan],
       [nan, nan, nan, nan, nan]], dtype=float32)
b = array([[0.556 , 0.4475, 0.529 , 0.4294, 0.5684],
       [0.539 , 0.519 , 0.545 , 0.4482, 0.5894],
       [0.465 , 0.5347, 0.571 , 0.473 , 0.568 ],
       [0.455 , 0.4866, 0.5166, 0.3848, 0.583 ]], dtype=float16)
precision = mlx.core.float16, rtol = 0.03, atol = 1e-05, message = None

    def assert_allclose(
        a: mx.array,
        b: mx.array,
        precision: mx.Dtype,
        rtol: float | None = None,
        atol: float | None = None,
        message: str | None = None,
    ):
        a = np.array(a)
        b = np.array(b)
        if precision == mx.float32:
            rtol = rtol or 1.0e-5
            atol = atol or 1.0e-8
        elif precision == mx.float16:
            rtol = rtol or 3.0e-2
            atol = atol or 1.0e-5
        else:
            raise ValueError(f"Unsupported precision: {precision}")
        assert a.shape == b.shape, f"shape mismatch: {a.shape} vs {b.shape}"
        if not np.allclose(a, b, rtol=rtol, atol=atol):
            diff = np.invert(np.isclose(a, b, rtol=rtol, atol=atol))
            if diff.size > 10000 and np.sum(diff) <= 3:
                # if only a small number of elements are different in a large array, probably fine
                return
            with np.printoptions(precision=3, suppress=True):
                print("a=", a)
                print("b=", b)
                print("diff_a=", a * diff)
                print("diff_b=", b * diff)
                print("diff_a_val=", a[diff])
                print("diff_b_val=", b[diff])
>               assert False, f"result mismatch: {message}"
                       ^^^^^
E               AssertionError: result mismatch: None

tests/utils.py:42: AssertionError
----------------------------- Captured stdout call -----------------------------
a= [[nan nan nan nan nan]
 [nan nan nan nan nan]
 [nan nan nan nan nan]
 [nan nan nan nan nan]]
b= [[0.556 0.448 0.529 0.429 0.568]
 [0.539 0.519 0.545 0.448 0.589]
 [0.465 0.535 0.571 0.473 0.568]
 [0.455 0.487 0.517 0.385 0.583]]
diff_a= [[nan nan nan nan nan]
 [nan nan nan nan nan]
 [nan nan nan nan nan]
 [nan nan nan nan nan]]
diff_b= [[0.556 0.448 0.529 0.429 0.568]
 [0.539 0.519 0.545 0.448 0.589]
 [0.465 0.535 0.571 0.473 0.568]
 [0.455 0.487 0.517 0.385 0.583]]
diff_a_val= [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan
 nan nan]
diff_b_val= [0.556 0.448 0.529 0.429 0.568 0.539 0.519 0.545 0.448 0.589 0.465 0.535
 0.571 0.473 0.568 0.455 0.487 0.517 0.385 0.583]
___________ test_task_1_simple_attention_scale_mask[batch_1-f32-cpu] ___________

stream = DeviceType.cpu, precision = mlx.core.float32, batch_dimension = 1

    @pytest.mark.parametrize("stream", AVAILABLE_STREAMS, ids=AVAILABLE_STREAMS_IDS)
    @pytest.mark.parametrize("precision", PRECISIONS, ids=PRECISION_IDS)
    @pytest.mark.parametrize(
        "batch_dimension", [0, 1, 2], ids=["batch_0", "batch_1", "batch_2"]
    )
    def test_task_1_simple_attention_scale_mask(
        stream: mx.Stream, precision: mx.Dtype, batch_dimension: int
    ):
        """
        Test if `scaled_dot_product_attention_simple` can process scale and mask correctly.
        """
        with mx.stream(stream):
            if batch_dimension == 0:
                BATCH_SIZE = ()
            elif batch_dimension == 1:
                BATCH_SIZE = (2, 3)
            elif batch_dimension == 2:
                BATCH_SIZE = (2, 3, 3)
            DIM_L = 4
            DIM_D = 5
            for _ in range(100):
                query = mx.random.uniform(
                    shape=(*BATCH_SIZE, DIM_L, DIM_D), dtype=precision
                )
                key = mx.random.uniform(shape=(*BATCH_SIZE, DIM_L, DIM_D), dtype=precision)
                value = mx.random.uniform(
                    shape=(*BATCH_SIZE, DIM_L, DIM_D), dtype=precision
                )
                mask = mx.random.uniform(shape=(*BATCH_SIZE, DIM_L, DIM_L), dtype=precision)
                scale = 0.5
                reference_output = mx.fast.scaled_dot_product_attention(
                    q=query.reshape(1, -1, DIM_L, DIM_D),
                    k=key.reshape(1, -1, DIM_L, DIM_D),
                    v=value.reshape(1, -1, DIM_L, DIM_D),
                    scale=scale,
                    mask=mask.reshape(1, -1, DIM_L, DIM_L),
                ).reshape(*BATCH_SIZE, DIM_L, DIM_D)
                user_output = scaled_dot_product_attention_simple(
                    query,
                    key,
                    value,
                    scale=scale,
                    mask=mask,
                )
>               assert_allclose(user_output, reference_output, precision=precision)

tests/test_week_1_day_1.py:108: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

a = array([[[[nan, nan, nan, nan, nan],
         [nan, nan, nan, nan, nan],
         [nan, nan, nan, nan, nan],
         [...   [nan, nan, nan, nan, nan],
         [nan, nan, nan, nan, nan],
         [nan, nan, nan, nan, nan]]]], dtype=float32)
b = array([[[[0.60191447, 0.6979946 , 0.5981369 , 0.14158027, 0.48627144],
         [0.50458276, 0.70710427, 0.60951   , 0...0.40598756, 0.76141983],
         [0.51990783, 0.5968216 , 0.48036066, 0.38190457, 0.73433805]]]],
      dtype=float32)
precision = mlx.core.float32, rtol = 1e-05, atol = 1e-08, message = None

    def assert_allclose(
        a: mx.array,
        b: mx.array,
        precision: mx.Dtype,
        rtol: float | None = None,
        atol: float | None = None,
        message: str | None = None,
    ):
        a = np.array(a)
        b = np.array(b)
        if precision == mx.float32:
            rtol = rtol or 1.0e-5
            atol = atol or 1.0e-8
        elif precision == mx.float16:
            rtol = rtol or 3.0e-2
            atol = atol or 1.0e-5
        else:
            raise ValueError(f"Unsupported precision: {precision}")
        assert a.shape == b.shape, f"shape mismatch: {a.shape} vs {b.shape}"
        if not np.allclose(a, b, rtol=rtol, atol=atol):
            diff = np.invert(np.isclose(a, b, rtol=rtol, atol=atol))
            if diff.size > 10000 and np.sum(diff) <= 3:
                # if only a small number of elements are different in a large array, probably fine
                return
            with np.printoptions(precision=3, suppress=True):
                print("a=", a)
                print("b=", b)
                print("diff_a=", a * diff)
                print("diff_b=", b * diff)
                print("diff_a_val=", a[diff])
                print("diff_b_val=", b[diff])
>               assert False, f"result mismatch: {message}"
                       ^^^^^
E               AssertionError: result mismatch: None

tests/utils.py:42: AssertionError
----------------------------- Captured stdout call -----------------------------
a= [[[[nan nan nan nan nan]
   [nan nan nan nan nan]
   [nan nan nan nan nan]
   [nan nan nan nan nan]]

  [[nan nan nan nan nan]
   [nan nan nan nan nan]
   [nan nan nan nan nan]
   [nan nan nan nan nan]]

  [[nan nan nan nan nan]
   [nan nan nan nan nan]
   [nan nan nan nan nan]
   [nan nan nan nan nan]]]


 [[[nan nan nan nan nan]
   [nan nan nan nan nan]
   [nan nan nan nan nan]
   [nan nan nan nan nan]]

  [[nan nan nan nan nan]
   [nan nan nan nan nan]
   [nan nan nan nan nan]
   [nan nan nan nan nan]]

  [[nan nan nan nan nan]
   [nan nan nan nan nan]
   [nan nan nan nan nan]
   [nan nan nan nan nan]]]]
b= [[[[0.602 0.698 0.598 0.142 0.486]
   [0.505 0.707 0.61  0.168 0.467]
   [0.538 0.624 0.472 0.128 0.44 ]
   [0.549 0.659 0.533 0.14  0.463]]

  [[0.581 0.393 0.685 0.757 0.522]
   [0.528 0.33  0.625 0.737 0.564]
   [0.522 0.379 0.615 0.713 0.491]
   [0.487 0.306 0.564 0.701 0.543]]

  [[0.555 0.415 0.597 0.451 0.576]
   [0.564 0.415 0.615 0.483 0.579]
   [0.553 0.416 0.594 0.446 0.574]
   [0.611 0.399 0.568 0.483 0.656]]]


 [[[0.536 0.486 0.581 0.488 0.485]
   [0.482 0.493 0.532 0.497 0.474]
   [0.47  0.484 0.58  0.462 0.466]
   [0.43  0.496 0.523 0.475 0.463]]

  [[0.649 0.478 0.351 0.253 0.548]
   [0.657 0.489 0.365 0.262 0.563]
   [0.632 0.492 0.4   0.267 0.498]
   [0.714 0.502 0.404 0.272 0.605]]

  [[0.447 0.688 0.465 0.38  0.695]
   [0.471 0.588 0.511 0.367 0.696]
   [0.538 0.694 0.412 0.406 0.761]
   [0.52  0.597 0.48  0.382 0.734]]]]
diff_a= [[[[nan nan nan nan nan]
   [nan nan nan nan nan]
   [nan nan nan nan nan]
   [nan nan nan nan nan]]

  [[nan nan nan nan nan]
   [nan nan nan nan nan]
   [nan nan nan nan nan]
   [nan nan nan nan nan]]

  [[nan nan nan nan nan]
   [nan nan nan nan nan]
   [nan nan nan nan nan]
   [nan nan nan nan nan]]]


 [[[nan nan nan nan nan]
   [nan nan nan nan nan]
   [nan nan nan nan nan]
   [nan nan nan nan nan]]

  [[nan nan nan nan nan]
   [nan nan nan nan nan]
   [nan nan nan nan nan]
   [nan nan nan nan nan]]

  [[nan nan nan nan nan]
   [nan nan nan nan nan]
   [nan nan nan nan nan]
   [nan nan nan nan nan]]]]
diff_b= [[[[0.602 0.698 0.598 0.142 0.486]
   [0.505 0.707 0.61  0.168 0.467]
   [0.538 0.624 0.472 0.128 0.44 ]
   [0.549 0.659 0.533 0.14  0.463]]

  [[0.581 0.393 0.685 0.757 0.522]
   [0.528 0.33  0.625 0.737 0.564]
   [0.522 0.379 0.615 0.713 0.491]
   [0.487 0.306 0.564 0.701 0.543]]

  [[0.555 0.415 0.597 0.451 0.576]
   [0.564 0.415 0.615 0.483 0.579]
   [0.553 0.416 0.594 0.446 0.574]
   [0.611 0.399 0.568 0.483 0.656]]]


 [[[0.536 0.486 0.581 0.488 0.485]
   [0.482 0.493 0.532 0.497 0.474]
   [0.47  0.484 0.58  0.462 0.466]
   [0.43  0.496 0.523 0.475 0.463]]

  [[0.649 0.478 0.351 0.253 0.548]
   [0.657 0.489 0.365 0.262 0.563]
   [0.632 0.492 0.4   0.267 0.498]
   [0.714 0.502 0.404 0.272 0.605]]

  [[0.447 0.688 0.465 0.38  0.695]
   [0.471 0.588 0.511 0.367 0.696]
   [0.538 0.694 0.412 0.406 0.761]
   [0.52  0.597 0.48  0.382 0.734]]]]
diff_a_val= [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan
 nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan
 nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan
 nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan
 nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan
 nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan
 nan nan nan nan nan nan nan nan nan nan nan nan]
diff_b_val= [0.602 0.698 0.598 0.142 0.486 0.505 0.707 0.61  0.168 0.467 0.538 0.624
 0.472 0.128 0.44  0.549 0.659 0.533 0.14  0.463 0.581 0.393 0.685 0.757
 0.522 0.528 0.33  0.625 0.737 0.564 0.522 0.379 0.615 0.713 0.491 0.487
 0.306 0.564 0.701 0.543 0.555 0.415 0.597 0.451 0.576 0.564 0.415 0.615
 0.483 0.579 0.553 0.416 0.594 0.446 0.574 0.611 0.399 0.568 0.483 0.656
 0.536 0.486 0.581 0.488 0.485 0.482 0.493 0.532 0.497 0.474 0.47  0.484
 0.58  0.462 0.466 0.43  0.496 0.523 0.475 0.463 0.649 0.478 0.351 0.253
 0.548 0.657 0.489 0.365 0.262 0.563 0.632 0.492 0.4   0.267 0.498 0.714
 0.502 0.404 0.272 0.605 0.447 0.688 0.465 0.38  0.695 0.471 0.588 0.511
 0.367 0.696 0.538 0.694 0.412 0.406 0.761 0.52  0.597 0.48  0.382 0.734]
___________ test_task_1_simple_attention_scale_mask[batch_1-f32-gpu] ___________

stream = DeviceType.gpu, precision = mlx.core.float32, batch_dimension = 1

    @pytest.mark.parametrize("stream", AVAILABLE_STREAMS, ids=AVAILABLE_STREAMS_IDS)
    @pytest.mark.parametrize("precision", PRECISIONS, ids=PRECISION_IDS)
    @pytest.mark.parametrize(
        "batch_dimension", [0, 1, 2], ids=["batch_0", "batch_1", "batch_2"]
    )
    def test_task_1_simple_attention_scale_mask(
        stream: mx.Stream, precision: mx.Dtype, batch_dimension: int
    ):
        """
        Test if `scaled_dot_product_attention_simple` can process scale and mask correctly.
        """
        with mx.stream(stream):
            if batch_dimension == 0:
                BATCH_SIZE = ()
            elif batch_dimension == 1:
                BATCH_SIZE = (2, 3)
            elif batch_dimension == 2:
                BATCH_SIZE = (2, 3, 3)
            DIM_L = 4
            DIM_D = 5
            for _ in range(100):
                query = mx.random.uniform(
                    shape=(*BATCH_SIZE, DIM_L, DIM_D), dtype=precision
                )
                key = mx.random.uniform(shape=(*BATCH_SIZE, DIM_L, DIM_D), dtype=precision)
                value = mx.random.uniform(
                    shape=(*BATCH_SIZE, DIM_L, DIM_D), dtype=precision
                )
                mask = mx.random.uniform(shape=(*BATCH_SIZE, DIM_L, DIM_L), dtype=precision)
                scale = 0.5
                reference_output = mx.fast.scaled_dot_product_attention(
                    q=query.reshape(1, -1, DIM_L, DIM_D),
                    k=key.reshape(1, -1, DIM_L, DIM_D),
                    v=value.reshape(1, -1, DIM_L, DIM_D),
                    scale=scale,
                    mask=mask.reshape(1, -1, DIM_L, DIM_L),
                ).reshape(*BATCH_SIZE, DIM_L, DIM_D)
                user_output = scaled_dot_product_attention_simple(
                    query,
                    key,
                    value,
                    scale=scale,
                    mask=mask,
                )
>               assert_allclose(user_output, reference_output, precision=precision)

tests/test_week_1_day_1.py:108: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

a = array([[[[nan, nan, nan, nan, nan],
         [nan, nan, nan, nan, nan],
         [nan, nan, nan, nan, nan],
         [...   [nan, nan, nan, nan, nan],
         [nan, nan, nan, nan, nan],
         [nan, nan, nan, nan, nan]]]], dtype=float32)
b = array([[[[0.6205318 , 0.5543992 , 0.39204344, 0.37664858, 0.6211983 ],
         [0.617747  , 0.5642321 , 0.3472584 , 0...0.25699183, 0.36558166],
         [0.3515531 , 0.42581528, 0.71164167, 0.24361825, 0.37075225]]]],
      dtype=float32)
precision = mlx.core.float32, rtol = 1e-05, atol = 1e-08, message = None

    def assert_allclose(
        a: mx.array,
        b: mx.array,
        precision: mx.Dtype,
        rtol: float | None = None,
        atol: float | None = None,
        message: str | None = None,
    ):
        a = np.array(a)
        b = np.array(b)
        if precision == mx.float32:
            rtol = rtol or 1.0e-5
            atol = atol or 1.0e-8
        elif precision == mx.float16:
            rtol = rtol or 3.0e-2
            atol = atol or 1.0e-5
        else:
            raise ValueError(f"Unsupported precision: {precision}")
        assert a.shape == b.shape, f"shape mismatch: {a.shape} vs {b.shape}"
        if not np.allclose(a, b, rtol=rtol, atol=atol):
            diff = np.invert(np.isclose(a, b, rtol=rtol, atol=atol))
            if diff.size > 10000 and np.sum(diff) <= 3:
                # if only a small number of elements are different in a large array, probably fine
                return
            with np.printoptions(precision=3, suppress=True):
                print("a=", a)
                print("b=", b)
                print("diff_a=", a * diff)
                print("diff_b=", b * diff)
                print("diff_a_val=", a[diff])
                print("diff_b_val=", b[diff])
>               assert False, f"result mismatch: {message}"
                       ^^^^^
E               AssertionError: result mismatch: None

tests/utils.py:42: AssertionError
----------------------------- Captured stdout call -----------------------------
a= [[[[nan nan nan nan nan]
   [nan nan nan nan nan]
   [nan nan nan nan nan]
   [nan nan nan nan nan]]

  [[nan nan nan nan nan]
   [nan nan nan nan nan]
   [nan nan nan nan nan]
   [nan nan nan nan nan]]

  [[nan nan nan nan nan]
   [nan nan nan nan nan]
   [nan nan nan nan nan]
   [nan nan nan nan nan]]]


 [[[nan nan nan nan nan]
   [nan nan nan nan nan]
   [nan nan nan nan nan]
   [nan nan nan nan nan]]

  [[nan nan nan nan nan]
   [nan nan nan nan nan]
   [nan nan nan nan nan]
   [nan nan nan nan nan]]

  [[nan nan nan nan nan]
   [nan nan nan nan nan]
   [nan nan nan nan nan]
   [nan nan nan nan nan]]]]
b= [[[[0.621 0.554 0.392 0.377 0.621]
   [0.618 0.564 0.347 0.407 0.701]
   [0.597 0.557 0.394 0.368 0.559]
   [0.568 0.57  0.381 0.36  0.515]]

  [[0.527 0.659 0.277 0.361 0.49 ]
   [0.529 0.56  0.448 0.544 0.514]
   [0.535 0.634 0.405 0.477 0.526]
   [0.545 0.472 0.462 0.6   0.481]]

  [[0.447 0.401 0.587 0.482 0.621]
   [0.434 0.405 0.608 0.458 0.628]
   [0.544 0.42  0.688 0.522 0.652]
   [0.453 0.464 0.694 0.464 0.616]]]


 [[[0.149 0.506 0.701 0.521 0.392]
   [0.169 0.518 0.768 0.522 0.355]
   [0.236 0.586 0.733 0.512 0.331]
   [0.159 0.497 0.752 0.467 0.432]]

  [[0.383 0.635 0.625 0.493 0.568]
   [0.547 0.501 0.582 0.523 0.623]
   [0.441 0.487 0.529 0.516 0.62 ]
   [0.531 0.608 0.663 0.508 0.585]]

  [[0.335 0.377 0.683 0.239 0.446]
   [0.402 0.388 0.659 0.2   0.452]
   [0.31  0.446 0.742 0.257 0.366]
   [0.352 0.426 0.712 0.244 0.371]]]]
diff_a= [[[[nan nan nan nan nan]
   [nan nan nan nan nan]
   [nan nan nan nan nan]
   [nan nan nan nan nan]]

  [[nan nan nan nan nan]
   [nan nan nan nan nan]
   [nan nan nan nan nan]
   [nan nan nan nan nan]]

  [[nan nan nan nan nan]
   [nan nan nan nan nan]
   [nan nan nan nan nan]
   [nan nan nan nan nan]]]


 [[[nan nan nan nan nan]
   [nan nan nan nan nan]
   [nan nan nan nan nan]
   [nan nan nan nan nan]]

  [[nan nan nan nan nan]
   [nan nan nan nan nan]
   [nan nan nan nan nan]
   [nan nan nan nan nan]]

  [[nan nan nan nan nan]
   [nan nan nan nan nan]
   [nan nan nan nan nan]
   [nan nan nan nan nan]]]]
diff_b= [[[[0.621 0.554 0.392 0.377 0.621]
   [0.618 0.564 0.347 0.407 0.701]
   [0.597 0.557 0.394 0.368 0.559]
   [0.568 0.57  0.381 0.36  0.515]]

  [[0.527 0.659 0.277 0.361 0.49 ]
   [0.529 0.56  0.448 0.544 0.514]
   [0.535 0.634 0.405 0.477 0.526]
   [0.545 0.472 0.462 0.6   0.481]]

  [[0.447 0.401 0.587 0.482 0.621]
   [0.434 0.405 0.608 0.458 0.628]
   [0.544 0.42  0.688 0.522 0.652]
   [0.453 0.464 0.694 0.464 0.616]]]


 [[[0.149 0.506 0.701 0.521 0.392]
   [0.169 0.518 0.768 0.522 0.355]
   [0.236 0.586 0.733 0.512 0.331]
   [0.159 0.497 0.752 0.467 0.432]]

  [[0.383 0.635 0.625 0.493 0.568]
   [0.547 0.501 0.582 0.523 0.623]
   [0.441 0.487 0.529 0.516 0.62 ]
   [0.531 0.608 0.663 0.508 0.585]]

  [[0.335 0.377 0.683 0.239 0.446]
   [0.402 0.388 0.659 0.2   0.452]
   [0.31  0.446 0.742 0.257 0.366]
   [0.352 0.426 0.712 0.244 0.371]]]]
diff_a_val= [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan
 nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan
 nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan
 nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan
 nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan
 nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan
 nan nan nan nan nan nan nan nan nan nan nan nan]
diff_b_val= [0.621 0.554 0.392 0.377 0.621 0.618 0.564 0.347 0.407 0.701 0.597 0.557
 0.394 0.368 0.559 0.568 0.57  0.381 0.36  0.515 0.527 0.659 0.277 0.361
 0.49  0.529 0.56  0.448 0.544 0.514 0.535 0.634 0.405 0.477 0.526 0.545
 0.472 0.462 0.6   0.481 0.447 0.401 0.587 0.482 0.621 0.434 0.405 0.608
 0.458 0.628 0.544 0.42  0.688 0.522 0.652 0.453 0.464 0.694 0.464 0.616
 0.149 0.506 0.701 0.521 0.392 0.169 0.518 0.768 0.522 0.355 0.236 0.586
 0.733 0.512 0.331 0.159 0.497 0.752 0.467 0.432 0.383 0.635 0.625 0.493
 0.568 0.547 0.501 0.582 0.523 0.623 0.441 0.487 0.529 0.516 0.62  0.531
 0.608 0.663 0.508 0.585 0.335 0.377 0.683 0.239 0.446 0.402 0.388 0.659
 0.2   0.452 0.31  0.446 0.742 0.257 0.366 0.352 0.426 0.712 0.244 0.371]
___________ test_task_1_simple_attention_scale_mask[batch_1-f16-cpu] ___________

stream = DeviceType.cpu, precision = mlx.core.float16, batch_dimension = 1

    @pytest.mark.parametrize("stream", AVAILABLE_STREAMS, ids=AVAILABLE_STREAMS_IDS)
    @pytest.mark.parametrize("precision", PRECISIONS, ids=PRECISION_IDS)
    @pytest.mark.parametrize(
        "batch_dimension", [0, 1, 2], ids=["batch_0", "batch_1", "batch_2"]
    )
    def test_task_1_simple_attention_scale_mask(
        stream: mx.Stream, precision: mx.Dtype, batch_dimension: int
    ):
        """
        Test if `scaled_dot_product_attention_simple` can process scale and mask correctly.
        """
        with mx.stream(stream):
            if batch_dimension == 0:
                BATCH_SIZE = ()
            elif batch_dimension == 1:
                BATCH_SIZE = (2, 3)
            elif batch_dimension == 2:
                BATCH_SIZE = (2, 3, 3)
            DIM_L = 4
            DIM_D = 5
            for _ in range(100):
                query = mx.random.uniform(
                    shape=(*BATCH_SIZE, DIM_L, DIM_D), dtype=precision
                )
                key = mx.random.uniform(shape=(*BATCH_SIZE, DIM_L, DIM_D), dtype=precision)
                value = mx.random.uniform(
                    shape=(*BATCH_SIZE, DIM_L, DIM_D), dtype=precision
                )
                mask = mx.random.uniform(shape=(*BATCH_SIZE, DIM_L, DIM_L), dtype=precision)
                scale = 0.5
                reference_output = mx.fast.scaled_dot_product_attention(
                    q=query.reshape(1, -1, DIM_L, DIM_D),
                    k=key.reshape(1, -1, DIM_L, DIM_D),
                    v=value.reshape(1, -1, DIM_L, DIM_D),
                    scale=scale,
                    mask=mask.reshape(1, -1, DIM_L, DIM_L),
                ).reshape(*BATCH_SIZE, DIM_L, DIM_D)
                user_output = scaled_dot_product_attention_simple(
                    query,
                    key,
                    value,
                    scale=scale,
                    mask=mask,
                )
>               assert_allclose(user_output, reference_output, precision=precision)

tests/test_week_1_day_1.py:108: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

a = array([[[[nan, nan, nan, nan, nan],
         [nan, nan, nan, nan, nan],
         [nan, nan, nan, nan, nan],
         [...   [nan, nan, nan, nan, nan],
         [nan, nan, nan, nan, nan],
         [nan, nan, nan, nan, nan]]]], dtype=float32)
b = array([[[[0.2001 , 0.582  , 0.5854 , 0.475  , 0.5063 ],
         [0.2238 , 0.561  , 0.589  , 0.4536 , 0.5244 ],
      ...0.4407 , 0.6577 , 0.6553 , 0.3086 , 0.636  ],
         [0.4058 , 0.6284 , 0.537  , 0.2117 , 0.6006 ]]]], dtype=float16)
precision = mlx.core.float16, rtol = 0.03, atol = 1e-05, message = None

    def assert_allclose(
        a: mx.array,
        b: mx.array,
        precision: mx.Dtype,
        rtol: float | None = None,
        atol: float | None = None,
        message: str | None = None,
    ):
        a = np.array(a)
        b = np.array(b)
        if precision == mx.float32:
            rtol = rtol or 1.0e-5
            atol = atol or 1.0e-8
        elif precision == mx.float16:
            rtol = rtol or 3.0e-2
            atol = atol or 1.0e-5
        else:
            raise ValueError(f"Unsupported precision: {precision}")
        assert a.shape == b.shape, f"shape mismatch: {a.shape} vs {b.shape}"
        if not np.allclose(a, b, rtol=rtol, atol=atol):
            diff = np.invert(np.isclose(a, b, rtol=rtol, atol=atol))
            if diff.size > 10000 and np.sum(diff) <= 3:
                # if only a small number of elements are different in a large array, probably fine
                return
            with np.printoptions(precision=3, suppress=True):
                print("a=", a)
                print("b=", b)
                print("diff_a=", a * diff)
                print("diff_b=", b * diff)
                print("diff_a_val=", a[diff])
                print("diff_b_val=", b[diff])
>               assert False, f"result mismatch: {message}"
                       ^^^^^
E               AssertionError: result mismatch: None

tests/utils.py:42: AssertionError
----------------------------- Captured stdout call -----------------------------
a= [[[[nan nan nan nan nan]
   [nan nan nan nan nan]
   [nan nan nan nan nan]
   [nan nan nan nan nan]]

  [[nan nan nan nan nan]
   [nan nan nan nan nan]
   [nan nan nan nan nan]
   [nan nan nan nan nan]]

  [[nan nan nan nan nan]
   [nan nan nan nan nan]
   [nan nan nan nan nan]
   [nan nan nan nan nan]]]


 [[[nan nan nan nan nan]
   [nan nan nan nan nan]
   [nan nan nan nan nan]
   [nan nan nan nan nan]]

  [[nan nan nan nan nan]
   [nan nan nan nan nan]
   [nan nan nan nan nan]
   [nan nan nan nan nan]]

  [[nan nan nan nan nan]
   [nan nan nan nan nan]
   [nan nan nan nan nan]
   [nan nan nan nan nan]]]]
b= [[[[0.2   0.582 0.585 0.475 0.506]
   [0.224 0.561 0.589 0.454 0.524]
   [0.244 0.542 0.565 0.502 0.529]
   [0.237 0.574 0.581 0.475 0.518]]

  [[0.507 0.709 0.393 0.495 0.307]
   [0.537 0.669 0.581 0.412 0.272]
   [0.527 0.688 0.462 0.452 0.285]
   [0.443 0.7   0.398 0.522 0.279]]

  [[0.477 0.338 0.422 0.642 0.408]
   [0.426 0.296 0.387 0.669 0.42 ]
   [0.583 0.416 0.509 0.623 0.365]
   [0.472 0.324 0.433 0.662 0.397]]]


 [[[0.462 0.533 0.699 0.42  0.342]
   [0.49  0.555 0.704 0.434 0.407]
   [0.458 0.518 0.712 0.447 0.342]
   [0.503 0.561 0.713 0.446 0.444]]

  [[0.432 0.047 0.529 0.504 0.494]
   [0.399 0.042 0.56  0.534 0.518]
   [0.334 0.037 0.542 0.566 0.521]
   [0.415 0.042 0.55  0.543 0.518]]

  [[0.407 0.614 0.534 0.241 0.627]
   [0.371 0.61  0.501 0.25  0.626]
   [0.441 0.658 0.655 0.309 0.636]
   [0.406 0.628 0.537 0.212 0.601]]]]
diff_a= [[[[nan nan nan nan nan]
   [nan nan nan nan nan]
   [nan nan nan nan nan]
   [nan nan nan nan nan]]

  [[nan nan nan nan nan]
   [nan nan nan nan nan]
   [nan nan nan nan nan]
   [nan nan nan nan nan]]

  [[nan nan nan nan nan]
   [nan nan nan nan nan]
   [nan nan nan nan nan]
   [nan nan nan nan nan]]]


 [[[nan nan nan nan nan]
   [nan nan nan nan nan]
   [nan nan nan nan nan]
   [nan nan nan nan nan]]

  [[nan nan nan nan nan]
   [nan nan nan nan nan]
   [nan nan nan nan nan]
   [nan nan nan nan nan]]

  [[nan nan nan nan nan]
   [nan nan nan nan nan]
   [nan nan nan nan nan]
   [nan nan nan nan nan]]]]
diff_b= [[[[0.2   0.582 0.585 0.475 0.506]
   [0.224 0.561 0.589 0.454 0.524]
   [0.244 0.542 0.565 0.502 0.529]
   [0.237 0.574 0.581 0.475 0.518]]

  [[0.507 0.709 0.393 0.495 0.307]
   [0.537 0.669 0.581 0.412 0.272]
   [0.527 0.688 0.462 0.452 0.285]
   [0.443 0.7   0.398 0.522 0.279]]

  [[0.477 0.338 0.422 0.642 0.408]
   [0.426 0.296 0.387 0.669 0.42 ]
   [0.583 0.416 0.509 0.623 0.365]
   [0.472 0.324 0.433 0.662 0.397]]]


 [[[0.462 0.533 0.699 0.42  0.342]
   [0.49  0.555 0.704 0.434 0.407]
   [0.458 0.518 0.712 0.447 0.342]
   [0.503 0.561 0.713 0.446 0.444]]

  [[0.432 0.047 0.529 0.504 0.494]
   [0.399 0.042 0.56  0.534 0.518]
   [0.334 0.037 0.542 0.566 0.521]
   [0.415 0.042 0.55  0.543 0.518]]

  [[0.407 0.614 0.534 0.241 0.627]
   [0.371 0.61  0.501 0.25  0.626]
   [0.441 0.658 0.655 0.309 0.636]
   [0.406 0.628 0.537 0.212 0.601]]]]
diff_a_val= [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan
 nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan
 nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan
 nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan
 nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan
 nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan
 nan nan nan nan nan nan nan nan nan nan nan nan]
diff_b_val= [0.2   0.582 0.585 0.475 0.506 0.224 0.561 0.589 0.454 0.524 0.244 0.542
 0.565 0.502 0.529 0.237 0.574 0.581 0.475 0.518 0.507 0.709 0.393 0.495
 0.307 0.537 0.669 0.581 0.412 0.272 0.527 0.688 0.462 0.452 0.285 0.443
 0.7   0.398 0.522 0.279 0.477 0.338 0.422 0.642 0.408 0.426 0.296 0.387
 0.669 0.42  0.583 0.416 0.509 0.623 0.365 0.472 0.324 0.433 0.662 0.397
 0.462 0.533 0.699 0.42  0.342 0.49  0.555 0.704 0.434 0.407 0.458 0.518
 0.712 0.447 0.342 0.503 0.561 0.713 0.446 0.444 0.432 0.047 0.529 0.504
 0.494 0.399 0.042 0.56  0.534 0.518 0.334 0.037 0.542 0.566 0.521 0.415
 0.042 0.55  0.543 0.518 0.407 0.614 0.534 0.241 0.627 0.371 0.61  0.501
 0.25  0.626 0.441 0.658 0.655 0.309 0.636 0.406 0.628 0.537 0.212 0.601]
___________ test_task_1_simple_attention_scale_mask[batch_1-f16-gpu] ___________

stream = DeviceType.gpu, precision = mlx.core.float16, batch_dimension = 1

    @pytest.mark.parametrize("stream", AVAILABLE_STREAMS, ids=AVAILABLE_STREAMS_IDS)
    @pytest.mark.parametrize("precision", PRECISIONS, ids=PRECISION_IDS)
    @pytest.mark.parametrize(
        "batch_dimension", [0, 1, 2], ids=["batch_0", "batch_1", "batch_2"]
    )
    def test_task_1_simple_attention_scale_mask(
        stream: mx.Stream, precision: mx.Dtype, batch_dimension: int
    ):
        """
        Test if `scaled_dot_product_attention_simple` can process scale and mask correctly.
        """
        with mx.stream(stream):
            if batch_dimension == 0:
                BATCH_SIZE = ()
            elif batch_dimension == 1:
                BATCH_SIZE = (2, 3)
            elif batch_dimension == 2:
                BATCH_SIZE = (2, 3, 3)
            DIM_L = 4
            DIM_D = 5
            for _ in range(100):
                query = mx.random.uniform(
                    shape=(*BATCH_SIZE, DIM_L, DIM_D), dtype=precision
                )
                key = mx.random.uniform(shape=(*BATCH_SIZE, DIM_L, DIM_D), dtype=precision)
                value = mx.random.uniform(
                    shape=(*BATCH_SIZE, DIM_L, DIM_D), dtype=precision
                )
                mask = mx.random.uniform(shape=(*BATCH_SIZE, DIM_L, DIM_L), dtype=precision)
                scale = 0.5
                reference_output = mx.fast.scaled_dot_product_attention(
                    q=query.reshape(1, -1, DIM_L, DIM_D),
                    k=key.reshape(1, -1, DIM_L, DIM_D),
                    v=value.reshape(1, -1, DIM_L, DIM_D),
                    scale=scale,
                    mask=mask.reshape(1, -1, DIM_L, DIM_L),
                ).reshape(*BATCH_SIZE, DIM_L, DIM_D)
                user_output = scaled_dot_product_attention_simple(
                    query,
                    key,
                    value,
                    scale=scale,
                    mask=mask,
                )
>               assert_allclose(user_output, reference_output, precision=precision)

tests/test_week_1_day_1.py:108: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

a = array([[[[nan, nan, nan, nan, nan],
         [nan, nan, nan, nan, nan],
         [nan, nan, nan, nan, nan],
         [...   [nan, nan, nan, nan, nan],
         [nan, nan, nan, nan, nan],
         [nan, nan, nan, nan, nan]]]], dtype=float32)
b = array([[[[0.6733, 0.547 , 0.5947, 0.4158, 0.3992],
         [0.6245, 0.5635, 0.5474, 0.4202, 0.4019],
         [0.672 ...         [0.513 , 0.561 , 0.4236, 0.6704, 0.4268],
         [0.5693, 0.4822, 0.48  , 0.639 , 0.451 ]]]], dtype=float16)
precision = mlx.core.float16, rtol = 0.03, atol = 1e-05, message = None

    def assert_allclose(
        a: mx.array,
        b: mx.array,
        precision: mx.Dtype,
        rtol: float | None = None,
        atol: float | None = None,
        message: str | None = None,
    ):
        a = np.array(a)
        b = np.array(b)
        if precision == mx.float32:
            rtol = rtol or 1.0e-5
            atol = atol or 1.0e-8
        elif precision == mx.float16:
            rtol = rtol or 3.0e-2
            atol = atol or 1.0e-5
        else:
            raise ValueError(f"Unsupported precision: {precision}")
        assert a.shape == b.shape, f"shape mismatch: {a.shape} vs {b.shape}"
        if not np.allclose(a, b, rtol=rtol, atol=atol):
            diff = np.invert(np.isclose(a, b, rtol=rtol, atol=atol))
            if diff.size > 10000 and np.sum(diff) <= 3:
                # if only a small number of elements are different in a large array, probably fine
                return
            with np.printoptions(precision=3, suppress=True):
                print("a=", a)
                print("b=", b)
                print("diff_a=", a * diff)
                print("diff_b=", b * diff)
                print("diff_a_val=", a[diff])
                print("diff_b_val=", b[diff])
>               assert False, f"result mismatch: {message}"
                       ^^^^^
E               AssertionError: result mismatch: None

tests/utils.py:42: AssertionError
----------------------------- Captured stdout call -----------------------------
a= [[[[nan nan nan nan nan]
   [nan nan nan nan nan]
   [nan nan nan nan nan]
   [nan nan nan nan nan]]

  [[nan nan nan nan nan]
   [nan nan nan nan nan]
   [nan nan nan nan nan]
   [nan nan nan nan nan]]

  [[nan nan nan nan nan]
   [nan nan nan nan nan]
   [nan nan nan nan nan]
   [nan nan nan nan nan]]]


 [[[nan nan nan nan nan]
   [nan nan nan nan nan]
   [nan nan nan nan nan]
   [nan nan nan nan nan]]

  [[nan nan nan nan nan]
   [nan nan nan nan nan]
   [nan nan nan nan nan]
   [nan nan nan nan nan]]

  [[nan nan nan nan nan]
   [nan nan nan nan nan]
   [nan nan nan nan nan]
   [nan nan nan nan nan]]]]
b= [[[[0.673 0.547 0.595 0.416 0.399]
   [0.625 0.563 0.547 0.42  0.402]
   [0.672 0.527 0.579 0.428 0.371]
   [0.594 0.584 0.527 0.408 0.388]]

  [[0.248 0.507 0.515 0.448 0.552]
   [0.216 0.579 0.458 0.376 0.538]
   [0.24  0.52  0.541 0.42  0.577]
   [0.208 0.5   0.437 0.453 0.455]]

  [[0.583 0.448 0.415 0.333 0.443]
   [0.558 0.466 0.43  0.292 0.446]
   [0.615 0.396 0.35  0.349 0.495]
   [0.55  0.484 0.451 0.304 0.421]]]


 [[[0.187 0.402 0.626 0.443 0.453]
   [0.165 0.394 0.585 0.408 0.393]
   [0.188 0.451 0.612 0.423 0.383]
   [0.177 0.423 0.604 0.425 0.397]]

  [[0.488 0.619 0.366 0.65  0.625]
   [0.485 0.622 0.356 0.685 0.594]
   [0.387 0.588 0.508 0.661 0.673]
   [0.482 0.653 0.34  0.703 0.605]]

  [[0.501 0.565 0.468 0.709 0.352]
   [0.557 0.538 0.458 0.642 0.435]
   [0.513 0.561 0.424 0.67  0.427]
   [0.569 0.482 0.48  0.639 0.451]]]]
diff_a= [[[[nan nan nan nan nan]
   [nan nan nan nan nan]
   [nan nan nan nan nan]
   [nan nan nan nan nan]]

  [[nan nan nan nan nan]
   [nan nan nan nan nan]
   [nan nan nan nan nan]
   [nan nan nan nan nan]]

  [[nan nan nan nan nan]
   [nan nan nan nan nan]
   [nan nan nan nan nan]
   [nan nan nan nan nan]]]


 [[[nan nan nan nan nan]
   [nan nan nan nan nan]
   [nan nan nan nan nan]
   [nan nan nan nan nan]]

  [[nan nan nan nan nan]
   [nan nan nan nan nan]
   [nan nan nan nan nan]
   [nan nan nan nan nan]]

  [[nan nan nan nan nan]
   [nan nan nan nan nan]
   [nan nan nan nan nan]
   [nan nan nan nan nan]]]]
diff_b= [[[[0.673 0.547 0.595 0.416 0.399]
   [0.625 0.563 0.547 0.42  0.402]
   [0.672 0.527 0.579 0.428 0.371]
   [0.594 0.584 0.527 0.408 0.388]]

  [[0.248 0.507 0.515 0.448 0.552]
   [0.216 0.579 0.458 0.376 0.538]
   [0.24  0.52  0.541 0.42  0.577]
   [0.208 0.5   0.437 0.453 0.455]]

  [[0.583 0.448 0.415 0.333 0.443]
   [0.558 0.466 0.43  0.292 0.446]
   [0.615 0.396 0.35  0.349 0.495]
   [0.55  0.484 0.451 0.304 0.421]]]


 [[[0.187 0.402 0.626 0.443 0.453]
   [0.165 0.394 0.585 0.408 0.393]
   [0.188 0.451 0.612 0.423 0.383]
   [0.177 0.423 0.604 0.425 0.397]]

  [[0.488 0.619 0.366 0.65  0.625]
   [0.485 0.622 0.356 0.685 0.594]
   [0.387 0.588 0.508 0.661 0.673]
   [0.482 0.653 0.34  0.703 0.605]]

  [[0.501 0.565 0.468 0.709 0.352]
   [0.557 0.538 0.458 0.642 0.435]
   [0.513 0.561 0.424 0.67  0.427]
   [0.569 0.482 0.48  0.639 0.451]]]]
diff_a_val= [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan
 nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan
 nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan
 nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan
 nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan
 nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan
 nan nan nan nan nan nan nan nan nan nan nan nan]
diff_b_val= [0.673 0.547 0.595 0.416 0.399 0.625 0.563 0.547 0.42  0.402 0.672 0.527
 0.579 0.428 0.371 0.594 0.584 0.527 0.408 0.388 0.248 0.507 0.515 0.448
 0.552 0.216 0.579 0.458 0.376 0.538 0.24  0.52  0.541 0.42  0.577 0.208
 0.5   0.437 0.453 0.455 0.583 0.448 0.415 0.333 0.443 0.558 0.466 0.43
 0.292 0.446 0.615 0.396 0.35  0.349 0.495 0.55  0.484 0.451 0.304 0.421
 0.187 0.402 0.626 0.443 0.453 0.165 0.394 0.585 0.408 0.393 0.188 0.451
 0.612 0.423 0.383 0.177 0.423 0.604 0.425 0.397 0.488 0.619 0.366 0.65
 0.625 0.485 0.622 0.356 0.685 0.594 0.387 0.588 0.508 0.661 0.673 0.482
 0.653 0.34  0.703 0.605 0.501 0.565 0.468 0.709 0.352 0.557 0.538 0.458
 0.642 0.435 0.513 0.561 0.424 0.67  0.427 0.569 0.482 0.48  0.639 0.451]
___________ test_task_1_simple_attention_scale_mask[batch_2-f32-cpu] ___________

stream = DeviceType.cpu, precision = mlx.core.float32, batch_dimension = 2

    @pytest.mark.parametrize("stream", AVAILABLE_STREAMS, ids=AVAILABLE_STREAMS_IDS)
    @pytest.mark.parametrize("precision", PRECISIONS, ids=PRECISION_IDS)
    @pytest.mark.parametrize(
        "batch_dimension", [0, 1, 2], ids=["batch_0", "batch_1", "batch_2"]
    )
    def test_task_1_simple_attention_scale_mask(
        stream: mx.Stream, precision: mx.Dtype, batch_dimension: int
    ):
        """
        Test if `scaled_dot_product_attention_simple` can process scale and mask correctly.
        """
        with mx.stream(stream):
            if batch_dimension == 0:
                BATCH_SIZE = ()
            elif batch_dimension == 1:
                BATCH_SIZE = (2, 3)
            elif batch_dimension == 2:
                BATCH_SIZE = (2, 3, 3)
            DIM_L = 4
            DIM_D = 5
            for _ in range(100):
                query = mx.random.uniform(
                    shape=(*BATCH_SIZE, DIM_L, DIM_D), dtype=precision
                )
                key = mx.random.uniform(shape=(*BATCH_SIZE, DIM_L, DIM_D), dtype=precision)
                value = mx.random.uniform(
                    shape=(*BATCH_SIZE, DIM_L, DIM_D), dtype=precision
                )
                mask = mx.random.uniform(shape=(*BATCH_SIZE, DIM_L, DIM_L), dtype=precision)
                scale = 0.5
                reference_output = mx.fast.scaled_dot_product_attention(
                    q=query.reshape(1, -1, DIM_L, DIM_D),
                    k=key.reshape(1, -1, DIM_L, DIM_D),
                    v=value.reshape(1, -1, DIM_L, DIM_D),
                    scale=scale,
                    mask=mask.reshape(1, -1, DIM_L, DIM_L),
                ).reshape(*BATCH_SIZE, DIM_L, DIM_D)
                user_output = scaled_dot_product_attention_simple(
                    query,
                    key,
                    value,
                    scale=scale,
                    mask=mask,
                )
>               assert_allclose(user_output, reference_output, precision=precision)

tests/test_week_1_day_1.py:108: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

a = array([[[[[nan, nan, nan, nan, nan],
          [nan, nan, nan, nan, nan],
          [nan, nan, nan, nan, nan],
       ...[nan, nan, nan, nan, nan],
          [nan, nan, nan, nan, nan],
          [nan, nan, nan, nan, nan]]]]], dtype=float32)
b = array([[[[[0.4300363 , 0.51516145, 0.5387465 , 0.5851135 , 0.3646177 ],
          [0.4918386 , 0.5986968 , 0.5519954 ,...5312359 , 0.5902656 ],
          [0.45903432, 0.3710515 , 0.23129643, 0.48206562, 0.532208  ]]]]],
      dtype=float32)
precision = mlx.core.float32, rtol = 1e-05, atol = 1e-08, message = None

    def assert_allclose(
        a: mx.array,
        b: mx.array,
        precision: mx.Dtype,
        rtol: float | None = None,
        atol: float | None = None,
        message: str | None = None,
    ):
        a = np.array(a)
        b = np.array(b)
        if precision == mx.float32:
            rtol = rtol or 1.0e-5
            atol = atol or 1.0e-8
        elif precision == mx.float16:
            rtol = rtol or 3.0e-2
            atol = atol or 1.0e-5
        else:
            raise ValueError(f"Unsupported precision: {precision}")
        assert a.shape == b.shape, f"shape mismatch: {a.shape} vs {b.shape}"
        if not np.allclose(a, b, rtol=rtol, atol=atol):
            diff = np.invert(np.isclose(a, b, rtol=rtol, atol=atol))
            if diff.size > 10000 and np.sum(diff) <= 3:
                # if only a small number of elements are different in a large array, probably fine
                return
            with np.printoptions(precision=3, suppress=True):
                print("a=", a)
                print("b=", b)
                print("diff_a=", a * diff)
                print("diff_b=", b * diff)
                print("diff_a_val=", a[diff])
                print("diff_b_val=", b[diff])
>               assert False, f"result mismatch: {message}"
                       ^^^^^
E               AssertionError: result mismatch: None

tests/utils.py:42: AssertionError
----------------------------- Captured stdout call -----------------------------
a= [[[[[nan nan nan nan nan]
    [nan nan nan nan nan]
    [nan nan nan nan nan]
    [nan nan nan nan nan]]

   [[nan nan nan nan nan]
    [nan nan nan nan nan]
    [nan nan nan nan nan]
    [nan nan nan nan nan]]

   [[nan nan nan nan nan]
    [nan nan nan nan nan]
    [nan nan nan nan nan]
    [nan nan nan nan nan]]]


  [[[nan nan nan nan nan]
    [nan nan nan nan nan]
    [nan nan nan nan nan]
    [nan nan nan nan nan]]

   [[nan nan nan nan nan]
    [nan nan nan nan nan]
    [nan nan nan nan nan]
    [nan nan nan nan nan]]

   [[nan nan nan nan nan]
    [nan nan nan nan nan]
    [nan nan nan nan nan]
    [nan nan nan nan nan]]]


  [[[nan nan nan nan nan]
    [nan nan nan nan nan]
    [nan nan nan nan nan]
    [nan nan nan nan nan]]

   [[nan nan nan nan nan]
    [nan nan nan nan nan]
    [nan nan nan nan nan]
    [nan nan nan nan nan]]

   [[nan nan nan nan nan]
    [nan nan nan nan nan]
    [nan nan nan nan nan]
    [nan nan nan nan nan]]]]



 [[[[nan nan nan nan nan]
    [nan nan nan nan nan]
    [nan nan nan nan nan]
    [nan nan nan nan nan]]

   [[nan nan nan nan nan]
    [nan nan nan nan nan]
    [nan nan nan nan nan]
    [nan nan nan nan nan]]

   [[nan nan nan nan nan]
    [nan nan nan nan nan]
    [nan nan nan nan nan]
    [nan nan nan nan nan]]]


  [[[nan nan nan nan nan]
    [nan nan nan nan nan]
    [nan nan nan nan nan]
    [nan nan nan nan nan]]

   [[nan nan nan nan nan]
    [nan nan nan nan nan]
    [nan nan nan nan nan]
    [nan nan nan nan nan]]

   [[nan nan nan nan nan]
    [nan nan nan nan nan]
    [nan nan nan nan nan]
    [nan nan nan nan nan]]]


  [[[nan nan nan nan nan]
    [nan nan nan nan nan]
    [nan nan nan nan nan]
    [nan nan nan nan nan]]

   [[nan nan nan nan nan]
    [nan nan nan nan nan]
    [nan nan nan nan nan]
    [nan nan nan nan nan]]

   [[nan nan nan nan nan]
    [nan nan nan nan nan]
    [nan nan nan nan nan]
    [nan nan nan nan nan]]]]]
b= [[[[[0.43  0.515 0.539 0.585 0.365]
    [0.492 0.599 0.552 0.542 0.388]
    [0.435 0.596 0.549 0.515 0.384]
    [0.468 0.528 0.537 0.585 0.37 ]]

   [[0.3   0.618 0.313 0.537 0.732]
    [0.284 0.656 0.328 0.519 0.725]
    [0.265 0.58  0.325 0.585 0.711]
    [0.347 0.544 0.303 0.541 0.763]]

   [[0.64  0.783 0.475 0.384 0.437]
    [0.634 0.729 0.388 0.401 0.491]
    [0.651 0.754 0.431 0.374 0.434]
    [0.637 0.703 0.364 0.387 0.482]]]


  [[[0.458 0.706 0.643 0.45  0.242]
    [0.526 0.63  0.619 0.433 0.245]
    [0.504 0.669 0.638 0.412 0.241]
    [0.513 0.668 0.646 0.396 0.24 ]]

   [[0.613 0.459 0.623 0.597 0.547]
    [0.616 0.441 0.627 0.61  0.553]
    [0.625 0.379 0.634 0.59  0.533]
    [0.633 0.31  0.648 0.611 0.539]]

   [[0.597 0.738 0.618 0.304 0.631]
    [0.579 0.743 0.566 0.261 0.588]
    [0.607 0.73  0.593 0.29  0.615]
    [0.582 0.745 0.552 0.238 0.579]]]


  [[[0.592 0.293 0.665 0.605 0.608]
    [0.639 0.283 0.692 0.625 0.582]
    [0.571 0.307 0.753 0.585 0.488]
    [0.634 0.276 0.773 0.665 0.595]]

   [[0.154 0.203 0.565 0.537 0.457]
    [0.112 0.223 0.548 0.554 0.392]
    [0.173 0.199 0.61  0.508 0.547]
    [0.12  0.223 0.557 0.536 0.422]]

   [[0.296 0.395 0.674 0.584 0.359]
    [0.368 0.327 0.573 0.598 0.471]
    [0.357 0.382 0.683 0.617 0.302]
    [0.3   0.407 0.661 0.605 0.317]]]]



 [[[[0.75  0.637 0.497 0.52  0.525]
    [0.8   0.676 0.576 0.559 0.543]
    [0.779 0.661 0.54  0.547 0.534]
    [0.795 0.679 0.557 0.56  0.549]]

   [[0.393 0.694 0.356 0.487 0.265]
    [0.325 0.599 0.347 0.498 0.214]
    [0.312 0.712 0.257 0.575 0.231]
    [0.378 0.675 0.322 0.547 0.262]]

   [[0.54  0.171 0.337 0.497 0.696]
    [0.552 0.227 0.347 0.455 0.725]
    [0.5   0.195 0.272 0.497 0.696]
    [0.577 0.213 0.311 0.445 0.728]]]


  [[[0.68  0.885 0.616 0.752 0.62 ]
    [0.599 0.813 0.637 0.779 0.523]
    [0.71  0.862 0.703 0.764 0.522]
    [0.7   0.871 0.679 0.774 0.539]]

   [[0.407 0.791 0.289 0.376 0.391]
    [0.425 0.804 0.313 0.374 0.406]
    [0.413 0.784 0.303 0.399 0.418]
    [0.315 0.705 0.22  0.459 0.393]]

   [[0.567 0.381 0.422 0.381 0.55 ]
    [0.536 0.41  0.425 0.369 0.546]
    [0.599 0.371 0.446 0.396 0.545]
    [0.545 0.412 0.352 0.307 0.518]]]


  [[[0.304 0.65  0.28  0.461 0.322]
    [0.291 0.639 0.302 0.48  0.318]
    [0.355 0.657 0.273 0.495 0.287]
    [0.385 0.672 0.287 0.465 0.344]]

   [[0.541 0.553 0.656 0.559 0.499]
    [0.541 0.544 0.659 0.557 0.493]
    [0.53  0.496 0.578 0.498 0.441]
    [0.539 0.511 0.66  0.548 0.472]]

   [[0.474 0.414 0.233 0.506 0.546]
    [0.447 0.426 0.23  0.467 0.564]
    [0.406 0.406 0.178 0.531 0.59 ]
    [0.459 0.371 0.231 0.482 0.532]]]]]
diff_a= [[[[[nan nan nan nan nan]
    [nan nan nan nan nan]
    [nan nan nan nan nan]
    [nan nan nan nan nan]]

   [[nan nan nan nan nan]
    [nan nan nan nan nan]
    [nan nan nan nan nan]
    [nan nan nan nan nan]]

   [[nan nan nan nan nan]
    [nan nan nan nan nan]
    [nan nan nan nan nan]
    [nan nan nan nan nan]]]


  [[[nan nan nan nan nan]
    [nan nan nan nan nan]
    [nan nan nan nan nan]
    [nan nan nan nan nan]]

   [[nan nan nan nan nan]
    [nan nan nan nan nan]
    [nan nan nan nan nan]
    [nan nan nan nan nan]]

   [[nan nan nan nan nan]
    [nan nan nan nan nan]
    [nan nan nan nan nan]
    [nan nan nan nan nan]]]


  [[[nan nan nan nan nan]
    [nan nan nan nan nan]
    [nan nan nan nan nan]
    [nan nan nan nan nan]]

   [[nan nan nan nan nan]
    [nan nan nan nan nan]
    [nan nan nan nan nan]
    [nan nan nan nan nan]]

   [[nan nan nan nan nan]
    [nan nan nan nan nan]
    [nan nan nan nan nan]
    [nan nan nan nan nan]]]]



 [[[[nan nan nan nan nan]
    [nan nan nan nan nan]
    [nan nan nan nan nan]
    [nan nan nan nan nan]]

   [[nan nan nan nan nan]
    [nan nan nan nan nan]
    [nan nan nan nan nan]
    [nan nan nan nan nan]]

   [[nan nan nan nan nan]
    [nan nan nan nan nan]
    [nan nan nan nan nan]
    [nan nan nan nan nan]]]


  [[[nan nan nan nan nan]
    [nan nan nan nan nan]
    [nan nan nan nan nan]
    [nan nan nan nan nan]]

   [[nan nan nan nan nan]
    [nan nan nan nan nan]
    [nan nan nan nan nan]
    [nan nan nan nan nan]]

   [[nan nan nan nan nan]
    [nan nan nan nan nan]
    [nan nan nan nan nan]
    [nan nan nan nan nan]]]


  [[[nan nan nan nan nan]
    [nan nan nan nan nan]
    [nan nan nan nan nan]
    [nan nan nan nan nan]]

   [[nan nan nan nan nan]
    [nan nan nan nan nan]
    [nan nan nan nan nan]
    [nan nan nan nan nan]]

   [[nan nan nan nan nan]
    [nan nan nan nan nan]
    [nan nan nan nan nan]
    [nan nan nan nan nan]]]]]
diff_b= [[[[[0.43  0.515 0.539 0.585 0.365]
    [0.492 0.599 0.552 0.542 0.388]
    [0.435 0.596 0.549 0.515 0.384]
    [0.468 0.528 0.537 0.585 0.37 ]]

   [[0.3   0.618 0.313 0.537 0.732]
    [0.284 0.656 0.328 0.519 0.725]
    [0.265 0.58  0.325 0.585 0.711]
    [0.347 0.544 0.303 0.541 0.763]]

   [[0.64  0.783 0.475 0.384 0.437]
    [0.634 0.729 0.388 0.401 0.491]
    [0.651 0.754 0.431 0.374 0.434]
    [0.637 0.703 0.364 0.387 0.482]]]


  [[[0.458 0.706 0.643 0.45  0.242]
    [0.526 0.63  0.619 0.433 0.245]
    [0.504 0.669 0.638 0.412 0.241]
    [0.513 0.668 0.646 0.396 0.24 ]]

   [[0.613 0.459 0.623 0.597 0.547]
    [0.616 0.441 0.627 0.61  0.553]
    [0.625 0.379 0.634 0.59  0.533]
    [0.633 0.31  0.648 0.611 0.539]]

   [[0.597 0.738 0.618 0.304 0.631]
    [0.579 0.743 0.566 0.261 0.588]
    [0.607 0.73  0.593 0.29  0.615]
    [0.582 0.745 0.552 0.238 0.579]]]


  [[[0.592 0.293 0.665 0.605 0.608]
    [0.639 0.283 0.692 0.625 0.582]
    [0.571 0.307 0.753 0.585 0.488]
    [0.634 0.276 0.773 0.665 0.595]]

   [[0.154 0.203 0.565 0.537 0.457]
    [0.112 0.223 0.548 0.554 0.392]
    [0.173 0.199 0.61  0.508 0.547]
    [0.12  0.223 0.557 0.536 0.422]]

   [[0.296 0.395 0.674 0.584 0.359]
    [0.368 0.327 0.573 0.598 0.471]
    [0.357 0.382 0.683 0.617 0.302]
    [0.3   0.407 0.661 0.605 0.317]]]]



 [[[[0.75  0.637 0.497 0.52  0.525]
    [0.8   0.676 0.576 0.559 0.543]
    [0.779 0.661 0.54  0.547 0.534]
    [0.795 0.679 0.557 0.56  0.549]]

   [[0.393 0.694 0.356 0.487 0.265]
    [0.325 0.599 0.347 0.498 0.214]
    [0.312 0.712 0.257 0.575 0.231]
    [0.378 0.675 0.322 0.547 0.262]]

   [[0.54  0.171 0.337 0.497 0.696]
    [0.552 0.227 0.347 0.455 0.725]
    [0.5   0.195 0.272 0.497 0.696]
    [0.577 0.213 0.311 0.445 0.728]]]


  [[[0.68  0.885 0.616 0.752 0.62 ]
    [0.599 0.813 0.637 0.779 0.523]
    [0.71  0.862 0.703 0.764 0.522]
    [0.7   0.871 0.679 0.774 0.539]]

   [[0.407 0.791 0.289 0.376 0.391]
    [0.425 0.804 0.313 0.374 0.406]
    [0.413 0.784 0.303 0.399 0.418]
    [0.315 0.705 0.22  0.459 0.393]]

   [[0.567 0.381 0.422 0.381 0.55 ]
    [0.536 0.41  0.425 0.369 0.546]
    [0.599 0.371 0.446 0.396 0.545]
    [0.545 0.412 0.352 0.307 0.518]]]


  [[[0.304 0.65  0.28  0.461 0.322]
    [0.291 0.639 0.302 0.48  0.318]
    [0.355 0.657 0.273 0.495 0.287]
    [0.385 0.672 0.287 0.465 0.344]]

   [[0.541 0.553 0.656 0.559 0.499]
    [0.541 0.544 0.659 0.557 0.493]
    [0.53  0.496 0.578 0.498 0.441]
    [0.539 0.511 0.66  0.548 0.472]]

   [[0.474 0.414 0.233 0.506 0.546]
    [0.447 0.426 0.23  0.467 0.564]
    [0.406 0.406 0.178 0.531 0.59 ]
    [0.459 0.371 0.231 0.482 0.532]]]]]
diff_a_val= [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan
 nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan
 nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan
 nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan
 nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan
 nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan
 nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan
 nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan
 nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan
 nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan
 nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan
 nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan
 nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan
 nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan
 nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan
 nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan
 nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan
 nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan
 nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan
 nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan]
diff_b_val= [0.43  0.515 0.539 0.585 0.365 0.492 0.599 0.552 0.542 0.388 0.435 0.596
 0.549 0.515 0.384 0.468 0.528 0.537 0.585 0.37  0.3   0.618 0.313 0.537
 0.732 0.284 0.656 0.328 0.519 0.725 0.265 0.58  0.325 0.585 0.711 0.347
 0.544 0.303 0.541 0.763 0.64  0.783 0.475 0.384 0.437 0.634 0.729 0.388
 0.401 0.491 0.651 0.754 0.431 0.374 0.434 0.637 0.703 0.364 0.387 0.482
 0.458 0.706 0.643 0.45  0.242 0.526 0.63  0.619 0.433 0.245 0.504 0.669
 0.638 0.412 0.241 0.513 0.668 0.646 0.396 0.24  0.613 0.459 0.623 0.597
 0.547 0.616 0.441 0.627 0.61  0.553 0.625 0.379 0.634 0.59  0.533 0.633
 0.31  0.648 0.611 0.539 0.597 0.738 0.618 0.304 0.631 0.579 0.743 0.566
 0.261 0.588 0.607 0.73  0.593 0.29  0.615 0.582 0.745 0.552 0.238 0.579
 0.592 0.293 0.665 0.605 0.608 0.639 0.283 0.692 0.625 0.582 0.571 0.307
 0.753 0.585 0.488 0.634 0.276 0.773 0.665 0.595 0.154 0.203 0.565 0.537
 0.457 0.112 0.223 0.548 0.554 0.392 0.173 0.199 0.61  0.508 0.547 0.12
 0.223 0.557 0.536 0.422 0.296 0.395 0.674 0.584 0.359 0.368 0.327 0.573
 0.598 0.471 0.357 0.382 0.683 0.617 0.302 0.3   0.407 0.661 0.605 0.317
 0.75  0.637 0.497 0.52  0.525 0.8   0.676 0.576 0.559 0.543 0.779 0.661
 0.54  0.547 0.534 0.795 0.679 0.557 0.56  0.549 0.393 0.694 0.356 0.487
 0.265 0.325 0.599 0.347 0.498 0.214 0.312 0.712 0.257 0.575 0.231 0.378
 0.675 0.322 0.547 0.262 0.54  0.171 0.337 0.497 0.696 0.552 0.227 0.347
 0.455 0.725 0.5   0.195 0.272 0.497 0.696 0.577 0.213 0.311 0.445 0.728
 0.68  0.885 0.616 0.752 0.62  0.599 0.813 0.637 0.779 0.523 0.71  0.862
 0.703 0.764 0.522 0.7   0.871 0.679 0.774 0.539 0.407 0.791 0.289 0.376
 0.391 0.425 0.804 0.313 0.374 0.406 0.413 0.784 0.303 0.399 0.418 0.315
 0.705 0.22  0.459 0.393 0.567 0.381 0.422 0.381 0.55  0.536 0.41  0.425
 0.369 0.546 0.599 0.371 0.446 0.396 0.545 0.545 0.412 0.352 0.307 0.518
 0.304 0.65  0.28  0.461 0.322 0.291 0.639 0.302 0.48  0.318 0.355 0.657
 0.273 0.495 0.287 0.385 0.672 0.287 0.465 0.344 0.541 0.553 0.656 0.559
 0.499 0.541 0.544 0.659 0.557 0.493 0.53  0.496 0.578 0.498 0.441 0.539
 0.511 0.66  0.548 0.472 0.474 0.414 0.233 0.506 0.546 0.447 0.426 0.23
 0.467 0.564 0.406 0.406 0.178 0.531 0.59  0.459 0.371 0.231 0.482 0.532]
___________ test_task_1_simple_attention_scale_mask[batch_2-f32-gpu] ___________

stream = DeviceType.gpu, precision = mlx.core.float32, batch_dimension = 2

    @pytest.mark.parametrize("stream", AVAILABLE_STREAMS, ids=AVAILABLE_STREAMS_IDS)
    @pytest.mark.parametrize("precision", PRECISIONS, ids=PRECISION_IDS)
    @pytest.mark.parametrize(
        "batch_dimension", [0, 1, 2], ids=["batch_0", "batch_1", "batch_2"]
    )
    def test_task_1_simple_attention_scale_mask(
        stream: mx.Stream, precision: mx.Dtype, batch_dimension: int
    ):
        """
        Test if `scaled_dot_product_attention_simple` can process scale and mask correctly.
        """
        with mx.stream(stream):
            if batch_dimension == 0:
                BATCH_SIZE = ()
            elif batch_dimension == 1:
                BATCH_SIZE = (2, 3)
            elif batch_dimension == 2:
                BATCH_SIZE = (2, 3, 3)
            DIM_L = 4
            DIM_D = 5
            for _ in range(100):
                query = mx.random.uniform(
                    shape=(*BATCH_SIZE, DIM_L, DIM_D), dtype=precision
                )
                key = mx.random.uniform(shape=(*BATCH_SIZE, DIM_L, DIM_D), dtype=precision)
                value = mx.random.uniform(
                    shape=(*BATCH_SIZE, DIM_L, DIM_D), dtype=precision
                )
                mask = mx.random.uniform(shape=(*BATCH_SIZE, DIM_L, DIM_L), dtype=precision)
                scale = 0.5
                reference_output = mx.fast.scaled_dot_product_attention(
                    q=query.reshape(1, -1, DIM_L, DIM_D),
                    k=key.reshape(1, -1, DIM_L, DIM_D),
                    v=value.reshape(1, -1, DIM_L, DIM_D),
                    scale=scale,
                    mask=mask.reshape(1, -1, DIM_L, DIM_L),
                ).reshape(*BATCH_SIZE, DIM_L, DIM_D)
                user_output = scaled_dot_product_attention_simple(
                    query,
                    key,
                    value,
                    scale=scale,
                    mask=mask,
                )
>               assert_allclose(user_output, reference_output, precision=precision)

tests/test_week_1_day_1.py:108: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

a = array([[[[[nan, nan, nan, nan, nan],
          [nan, nan, nan, nan, nan],
          [nan, nan, nan, nan, nan],
       ...[nan, nan, nan, nan, nan],
          [nan, nan, nan, nan, nan],
          [nan, nan, nan, nan, nan]]]]], dtype=float32)
b = array([[[[[0.35047913, 0.6218495 , 0.31640857, 0.39410272, 0.7821971 ],
          [0.39871797, 0.577955  , 0.3556578 ,...4093801 , 0.7539336 ],
          [0.6722643 , 0.5766854 , 0.5808071 , 0.39909866, 0.715883  ]]]]],
      dtype=float32)
precision = mlx.core.float32, rtol = 1e-05, atol = 1e-08, message = None

    def assert_allclose(
        a: mx.array,
        b: mx.array,
        precision: mx.Dtype,
        rtol: float | None = None,
        atol: float | None = None,
        message: str | None = None,
    ):
        a = np.array(a)
        b = np.array(b)
        if precision == mx.float32:
            rtol = rtol or 1.0e-5
            atol = atol or 1.0e-8
        elif precision == mx.float16:
            rtol = rtol or 3.0e-2
            atol = atol or 1.0e-5
        else:
            raise ValueError(f"Unsupported precision: {precision}")
        assert a.shape == b.shape, f"shape mismatch: {a.shape} vs {b.shape}"
        if not np.allclose(a, b, rtol=rtol, atol=atol):
            diff = np.invert(np.isclose(a, b, rtol=rtol, atol=atol))
            if diff.size > 10000 and np.sum(diff) <= 3:
                # if only a small number of elements are different in a large array, probably fine
                return
            with np.printoptions(precision=3, suppress=True):
                print("a=", a)
                print("b=", b)
                print("diff_a=", a * diff)
                print("diff_b=", b * diff)
                print("diff_a_val=", a[diff])
                print("diff_b_val=", b[diff])
>               assert False, f"result mismatch: {message}"
                       ^^^^^
E               AssertionError: result mismatch: None

tests/utils.py:42: AssertionError
----------------------------- Captured stdout call -----------------------------
a= [[[[[nan nan nan nan nan]
    [nan nan nan nan nan]
    [nan nan nan nan nan]
    [nan nan nan nan nan]]

   [[nan nan nan nan nan]
    [nan nan nan nan nan]
    [nan nan nan nan nan]
    [nan nan nan nan nan]]

   [[nan nan nan nan nan]
    [nan nan nan nan nan]
    [nan nan nan nan nan]
    [nan nan nan nan nan]]]


  [[[nan nan nan nan nan]
    [nan nan nan nan nan]
    [nan nan nan nan nan]
    [nan nan nan nan nan]]

   [[nan nan nan nan nan]
    [nan nan nan nan nan]
    [nan nan nan nan nan]
    [nan nan nan nan nan]]

   [[nan nan nan nan nan]
    [nan nan nan nan nan]
    [nan nan nan nan nan]
    [nan nan nan nan nan]]]


  [[[nan nan nan nan nan]
    [nan nan nan nan nan]
    [nan nan nan nan nan]
    [nan nan nan nan nan]]

   [[nan nan nan nan nan]
    [nan nan nan nan nan]
    [nan nan nan nan nan]
    [nan nan nan nan nan]]

   [[nan nan nan nan nan]
    [nan nan nan nan nan]
    [nan nan nan nan nan]
    [nan nan nan nan nan]]]]



 [[[[nan nan nan nan nan]
    [nan nan nan nan nan]
    [nan nan nan nan nan]
    [nan nan nan nan nan]]

   [[nan nan nan nan nan]
    [nan nan nan nan nan]
    [nan nan nan nan nan]
    [nan nan nan nan nan]]

   [[nan nan nan nan nan]
    [nan nan nan nan nan]
    [nan nan nan nan nan]
    [nan nan nan nan nan]]]


  [[[nan nan nan nan nan]
    [nan nan nan nan nan]
    [nan nan nan nan nan]
    [nan nan nan nan nan]]

   [[nan nan nan nan nan]
    [nan nan nan nan nan]
    [nan nan nan nan nan]
    [nan nan nan nan nan]]

   [[nan nan nan nan nan]
    [nan nan nan nan nan]
    [nan nan nan nan nan]
    [nan nan nan nan nan]]]


  [[[nan nan nan nan nan]
    [nan nan nan nan nan]
    [nan nan nan nan nan]
    [nan nan nan nan nan]]

   [[nan nan nan nan nan]
    [nan nan nan nan nan]
    [nan nan nan nan nan]
    [nan nan nan nan nan]]

   [[nan nan nan nan nan]
    [nan nan nan nan nan]
    [nan nan nan nan nan]
    [nan nan nan nan nan]]]]]
b= [[[[[0.35  0.622 0.316 0.394 0.782]
    [0.399 0.578 0.356 0.417 0.728]
    [0.424 0.583 0.348 0.411 0.757]
    [0.458 0.537 0.321 0.449 0.739]]

   [[0.775 0.601 0.346 0.196 0.363]
    [0.749 0.5   0.467 0.255 0.59 ]
    [0.725 0.582 0.371 0.229 0.455]
    [0.673 0.561 0.384 0.254 0.489]]

   [[0.571 0.564 0.604 0.333 0.57 ]
    [0.536 0.624 0.662 0.307 0.623]
    [0.457 0.662 0.604 0.305 0.557]
    [0.467 0.645 0.581 0.308 0.538]]]


  [[[0.598 0.53  0.413 0.55  0.441]
    [0.594 0.487 0.369 0.547 0.497]
    [0.615 0.476 0.347 0.585 0.482]
    [0.668 0.464 0.344 0.552 0.44 ]]

   [[0.392 0.638 0.389 0.335 0.595]
    [0.397 0.585 0.433 0.387 0.637]
    [0.408 0.604 0.355 0.353 0.559]
    [0.405 0.529 0.36  0.411 0.564]]

   [[0.509 0.85  0.55  0.412 0.536]
    [0.566 0.849 0.452 0.517 0.493]
    [0.533 0.844 0.476 0.486 0.541]
    [0.522 0.848 0.515 0.437 0.517]]]


  [[[0.338 0.409 0.44  0.542 0.723]
    [0.365 0.434 0.499 0.535 0.773]
    [0.357 0.423 0.429 0.529 0.712]
    [0.337 0.408 0.422 0.541 0.707]]

   [[0.46  0.245 0.703 0.361 0.58 ]
    [0.465 0.26  0.744 0.353 0.603]
    [0.437 0.247 0.695 0.352 0.571]
    [0.485 0.212 0.714 0.408 0.616]]

   [[0.595 0.485 0.231 0.546 0.479]
    [0.662 0.473 0.259 0.508 0.496]
    [0.63  0.494 0.258 0.536 0.44 ]
    [0.701 0.499 0.269 0.514 0.468]]]]



 [[[[0.53  0.563 0.284 0.529 0.37 ]
    [0.553 0.586 0.236 0.553 0.343]
    [0.626 0.616 0.281 0.409 0.268]
    [0.505 0.556 0.293 0.553 0.392]]

   [[0.542 0.739 0.392 0.412 0.542]
    [0.437 0.75  0.262 0.497 0.567]
    [0.522 0.712 0.345 0.471 0.526]
    [0.488 0.719 0.35  0.473 0.536]]

   [[0.522 0.381 0.393 0.488 0.511]
    [0.546 0.431 0.456 0.467 0.494]
    [0.521 0.323 0.387 0.548 0.557]
    [0.524 0.379 0.416 0.499 0.513]]]


  [[[0.606 0.763 0.599 0.416 0.644]
    [0.591 0.788 0.596 0.443 0.641]
    [0.662 0.753 0.597 0.408 0.608]
    [0.58  0.779 0.581 0.431 0.672]]

   [[0.598 0.396 0.624 0.648 0.665]
    [0.597 0.434 0.651 0.686 0.681]
    [0.54  0.458 0.609 0.728 0.712]
    [0.633 0.382 0.651 0.622 0.647]]

   [[0.589 0.395 0.649 0.355 0.73 ]
    [0.525 0.508 0.616 0.28  0.773]
    [0.539 0.509 0.598 0.224 0.786]
    [0.508 0.414 0.649 0.308 0.724]]]


  [[[0.554 0.446 0.495 0.448 0.548]
    [0.564 0.412 0.531 0.538 0.476]
    [0.527 0.425 0.479 0.507 0.514]
    [0.653 0.458 0.493 0.47  0.509]]

   [[0.483 0.505 0.556 0.083 0.358]
    [0.485 0.48  0.546 0.082 0.35 ]
    [0.511 0.489 0.618 0.068 0.282]
    [0.399 0.516 0.56  0.073 0.439]]

   [[0.681 0.582 0.637 0.381 0.71 ]
    [0.699 0.506 0.625 0.373 0.632]
    [0.667 0.591 0.57  0.409 0.754]
    [0.672 0.577 0.581 0.399 0.716]]]]]
diff_a= [[[[[nan nan nan nan nan]
    [nan nan nan nan nan]
    [nan nan nan nan nan]
    [nan nan nan nan nan]]

   [[nan nan nan nan nan]
    [nan nan nan nan nan]
    [nan nan nan nan nan]
    [nan nan nan nan nan]]

   [[nan nan nan nan nan]
    [nan nan nan nan nan]
    [nan nan nan nan nan]
    [nan nan nan nan nan]]]


  [[[nan nan nan nan nan]
    [nan nan nan nan nan]
    [nan nan nan nan nan]
    [nan nan nan nan nan]]

   [[nan nan nan nan nan]
    [nan nan nan nan nan]
    [nan nan nan nan nan]
    [nan nan nan nan nan]]

   [[nan nan nan nan nan]
    [nan nan nan nan nan]
    [nan nan nan nan nan]
    [nan nan nan nan nan]]]


  [[[nan nan nan nan nan]
    [nan nan nan nan nan]
    [nan nan nan nan nan]
    [nan nan nan nan nan]]

   [[nan nan nan nan nan]
    [nan nan nan nan nan]
    [nan nan nan nan nan]
    [nan nan nan nan nan]]

   [[nan nan nan nan nan]
    [nan nan nan nan nan]
    [nan nan nan nan nan]
    [nan nan nan nan nan]]]]



 [[[[nan nan nan nan nan]
    [nan nan nan nan nan]
    [nan nan nan nan nan]
    [nan nan nan nan nan]]

   [[nan nan nan nan nan]
    [nan nan nan nan nan]
    [nan nan nan nan nan]
    [nan nan nan nan nan]]

   [[nan nan nan nan nan]
    [nan nan nan nan nan]
    [nan nan nan nan nan]
    [nan nan nan nan nan]]]


  [[[nan nan nan nan nan]
    [nan nan nan nan nan]
    [nan nan nan nan nan]
    [nan nan nan nan nan]]

   [[nan nan nan nan nan]
    [nan nan nan nan nan]
    [nan nan nan nan nan]
    [nan nan nan nan nan]]

   [[nan nan nan nan nan]
    [nan nan nan nan nan]
    [nan nan nan nan nan]
    [nan nan nan nan nan]]]


  [[[nan nan nan nan nan]
    [nan nan nan nan nan]
    [nan nan nan nan nan]
    [nan nan nan nan nan]]

   [[nan nan nan nan nan]
    [nan nan nan nan nan]
    [nan nan nan nan nan]
    [nan nan nan nan nan]]

   [[nan nan nan nan nan]
    [nan nan nan nan nan]
    [nan nan nan nan nan]
    [nan nan nan nan nan]]]]]
diff_b= [[[[[0.35  0.622 0.316 0.394 0.782]
    [0.399 0.578 0.356 0.417 0.728]
    [0.424 0.583 0.348 0.411 0.757]
    [0.458 0.537 0.321 0.449 0.739]]

   [[0.775 0.601 0.346 0.196 0.363]
    [0.749 0.5   0.467 0.255 0.59 ]
    [0.725 0.582 0.371 0.229 0.455]
    [0.673 0.561 0.384 0.254 0.489]]

   [[0.571 0.564 0.604 0.333 0.57 ]
    [0.536 0.624 0.662 0.307 0.623]
    [0.457 0.662 0.604 0.305 0.557]
    [0.467 0.645 0.581 0.308 0.538]]]


  [[[0.598 0.53  0.413 0.55  0.441]
    [0.594 0.487 0.369 0.547 0.497]
    [0.615 0.476 0.347 0.585 0.482]
    [0.668 0.464 0.344 0.552 0.44 ]]

   [[0.392 0.638 0.389 0.335 0.595]
    [0.397 0.585 0.433 0.387 0.637]
    [0.408 0.604 0.355 0.353 0.559]
    [0.405 0.529 0.36  0.411 0.564]]

   [[0.509 0.85  0.55  0.412 0.536]
    [0.566 0.849 0.452 0.517 0.493]
    [0.533 0.844 0.476 0.486 0.541]
    [0.522 0.848 0.515 0.437 0.517]]]


  [[[0.338 0.409 0.44  0.542 0.723]
    [0.365 0.434 0.499 0.535 0.773]
    [0.357 0.423 0.429 0.529 0.712]
    [0.337 0.408 0.422 0.541 0.707]]

   [[0.46  0.245 0.703 0.361 0.58 ]
    [0.465 0.26  0.744 0.353 0.603]
    [0.437 0.247 0.695 0.352 0.571]
    [0.485 0.212 0.714 0.408 0.616]]

   [[0.595 0.485 0.231 0.546 0.479]
    [0.662 0.473 0.259 0.508 0.496]
    [0.63  0.494 0.258 0.536 0.44 ]
    [0.701 0.499 0.269 0.514 0.468]]]]



 [[[[0.53  0.563 0.284 0.529 0.37 ]
    [0.553 0.586 0.236 0.553 0.343]
    [0.626 0.616 0.281 0.409 0.268]
    [0.505 0.556 0.293 0.553 0.392]]

   [[0.542 0.739 0.392 0.412 0.542]
    [0.437 0.75  0.262 0.497 0.567]
    [0.522 0.712 0.345 0.471 0.526]
    [0.488 0.719 0.35  0.473 0.536]]

   [[0.522 0.381 0.393 0.488 0.511]
    [0.546 0.431 0.456 0.467 0.494]
    [0.521 0.323 0.387 0.548 0.557]
    [0.524 0.379 0.416 0.499 0.513]]]


  [[[0.606 0.763 0.599 0.416 0.644]
    [0.591 0.788 0.596 0.443 0.641]
    [0.662 0.753 0.597 0.408 0.608]
    [0.58  0.779 0.581 0.431 0.672]]

   [[0.598 0.396 0.624 0.648 0.665]
    [0.597 0.434 0.651 0.686 0.681]
    [0.54  0.458 0.609 0.728 0.712]
    [0.633 0.382 0.651 0.622 0.647]]

   [[0.589 0.395 0.649 0.355 0.73 ]
    [0.525 0.508 0.616 0.28  0.773]
    [0.539 0.509 0.598 0.224 0.786]
    [0.508 0.414 0.649 0.308 0.724]]]


  [[[0.554 0.446 0.495 0.448 0.548]
    [0.564 0.412 0.531 0.538 0.476]
    [0.527 0.425 0.479 0.507 0.514]
    [0.653 0.458 0.493 0.47  0.509]]

   [[0.483 0.505 0.556 0.083 0.358]
    [0.485 0.48  0.546 0.082 0.35 ]
    [0.511 0.489 0.618 0.068 0.282]
    [0.399 0.516 0.56  0.073 0.439]]

   [[0.681 0.582 0.637 0.381 0.71 ]
    [0.699 0.506 0.625 0.373 0.632]
    [0.667 0.591 0.57  0.409 0.754]
    [0.672 0.577 0.581 0.399 0.716]]]]]
diff_a_val= [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan
 nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan
 nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan
 nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan
 nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan
 nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan
 nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan
 nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan
 nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan
 nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan
 nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan
 nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan
 nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan
 nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan
 nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan
 nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan
 nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan
 nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan
 nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan
 nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan]
diff_b_val= [0.35  0.622 0.316 0.394 0.782 0.399 0.578 0.356 0.417 0.728 0.424 0.583
 0.348 0.411 0.757 0.458 0.537 0.321 0.449 0.739 0.775 0.601 0.346 0.196
 0.363 0.749 0.5   0.467 0.255 0.59  0.725 0.582 0.371 0.229 0.455 0.673
 0.561 0.384 0.254 0.489 0.571 0.564 0.604 0.333 0.57  0.536 0.624 0.662
 0.307 0.623 0.457 0.662 0.604 0.305 0.557 0.467 0.645 0.581 0.308 0.538
 0.598 0.53  0.413 0.55  0.441 0.594 0.487 0.369 0.547 0.497 0.615 0.476
 0.347 0.585 0.482 0.668 0.464 0.344 0.552 0.44  0.392 0.638 0.389 0.335
 0.595 0.397 0.585 0.433 0.387 0.637 0.408 0.604 0.355 0.353 0.559 0.405
 0.529 0.36  0.411 0.564 0.509 0.85  0.55  0.412 0.536 0.566 0.849 0.452
 0.517 0.493 0.533 0.844 0.476 0.486 0.541 0.522 0.848 0.515 0.437 0.517
 0.338 0.409 0.44  0.542 0.723 0.365 0.434 0.499 0.535 0.773 0.357 0.423
 0.429 0.529 0.712 0.337 0.408 0.422 0.541 0.707 0.46  0.245 0.703 0.361
 0.58  0.465 0.26  0.744 0.353 0.603 0.437 0.247 0.695 0.352 0.571 0.485
 0.212 0.714 0.408 0.616 0.595 0.485 0.231 0.546 0.479 0.662 0.473 0.259
 0.508 0.496 0.63  0.494 0.258 0.536 0.44  0.701 0.499 0.269 0.514 0.468
 0.53  0.563 0.284 0.529 0.37  0.553 0.586 0.236 0.553 0.343 0.626 0.616
 0.281 0.409 0.268 0.505 0.556 0.293 0.553 0.392 0.542 0.739 0.392 0.412
 0.542 0.437 0.75  0.262 0.497 0.567 0.522 0.712 0.345 0.471 0.526 0.488
 0.719 0.35  0.473 0.536 0.522 0.381 0.393 0.488 0.511 0.546 0.431 0.456
 0.467 0.494 0.521 0.323 0.387 0.548 0.557 0.524 0.379 0.416 0.499 0.513
 0.606 0.763 0.599 0.416 0.644 0.591 0.788 0.596 0.443 0.641 0.662 0.753
 0.597 0.408 0.608 0.58  0.779 0.581 0.431 0.672 0.598 0.396 0.624 0.648
 0.665 0.597 0.434 0.651 0.686 0.681 0.54  0.458 0.609 0.728 0.712 0.633
 0.382 0.651 0.622 0.647 0.589 0.395 0.649 0.355 0.73  0.525 0.508 0.616
 0.28  0.773 0.539 0.509 0.598 0.224 0.786 0.508 0.414 0.649 0.308 0.724
 0.554 0.446 0.495 0.448 0.548 0.564 0.412 0.531 0.538 0.476 0.527 0.425
 0.479 0.507 0.514 0.653 0.458 0.493 0.47  0.509 0.483 0.505 0.556 0.083
 0.358 0.485 0.48  0.546 0.082 0.35  0.511 0.489 0.618 0.068 0.282 0.399
 0.516 0.56  0.073 0.439 0.681 0.582 0.637 0.381 0.71  0.699 0.506 0.625
 0.373 0.632 0.667 0.591 0.57  0.409 0.754 0.672 0.577 0.581 0.399 0.716]
___________ test_task_1_simple_attention_scale_mask[batch_2-f16-cpu] ___________

stream = DeviceType.cpu, precision = mlx.core.float16, batch_dimension = 2

    @pytest.mark.parametrize("stream", AVAILABLE_STREAMS, ids=AVAILABLE_STREAMS_IDS)
    @pytest.mark.parametrize("precision", PRECISIONS, ids=PRECISION_IDS)
    @pytest.mark.parametrize(
        "batch_dimension", [0, 1, 2], ids=["batch_0", "batch_1", "batch_2"]
    )
    def test_task_1_simple_attention_scale_mask(
        stream: mx.Stream, precision: mx.Dtype, batch_dimension: int
    ):
        """
        Test if `scaled_dot_product_attention_simple` can process scale and mask correctly.
        """
        with mx.stream(stream):
            if batch_dimension == 0:
                BATCH_SIZE = ()
            elif batch_dimension == 1:
                BATCH_SIZE = (2, 3)
            elif batch_dimension == 2:
                BATCH_SIZE = (2, 3, 3)
            DIM_L = 4
            DIM_D = 5
            for _ in range(100):
                query = mx.random.uniform(
                    shape=(*BATCH_SIZE, DIM_L, DIM_D), dtype=precision
                )
                key = mx.random.uniform(shape=(*BATCH_SIZE, DIM_L, DIM_D), dtype=precision)
                value = mx.random.uniform(
                    shape=(*BATCH_SIZE, DIM_L, DIM_D), dtype=precision
                )
                mask = mx.random.uniform(shape=(*BATCH_SIZE, DIM_L, DIM_L), dtype=precision)
                scale = 0.5
                reference_output = mx.fast.scaled_dot_product_attention(
                    q=query.reshape(1, -1, DIM_L, DIM_D),
                    k=key.reshape(1, -1, DIM_L, DIM_D),
                    v=value.reshape(1, -1, DIM_L, DIM_D),
                    scale=scale,
                    mask=mask.reshape(1, -1, DIM_L, DIM_L),
                ).reshape(*BATCH_SIZE, DIM_L, DIM_D)
                user_output = scaled_dot_product_attention_simple(
                    query,
                    key,
                    value,
                    scale=scale,
                    mask=mask,
                )
>               assert_allclose(user_output, reference_output, precision=precision)

tests/test_week_1_day_1.py:108: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

a = array([[[[[nan, nan, nan, nan, nan],
          [nan, nan, nan, nan, nan],
          [nan, nan, nan, nan, nan],
       ...[nan, nan, nan, nan, nan],
          [nan, nan, nan, nan, nan],
          [nan, nan, nan, nan, nan]]]]], dtype=float32)
b = array([[[[[0.4805, 0.624 , 0.3499, 0.725 , 0.44  ],
          [0.575 , 0.688 , 0.4297, 0.5854, 0.4666],
          [0.5...       [0.2139, 0.4563, 0.7397, 0.2705, 0.6484],
          [0.1826, 0.4995, 0.7075, 0.3477, 0.613 ]]]]], dtype=float16)
precision = mlx.core.float16, rtol = 0.03, atol = 1e-05, message = None

    def assert_allclose(
        a: mx.array,
        b: mx.array,
        precision: mx.Dtype,
        rtol: float | None = None,
        atol: float | None = None,
        message: str | None = None,
    ):
        a = np.array(a)
        b = np.array(b)
        if precision == mx.float32:
            rtol = rtol or 1.0e-5
            atol = atol or 1.0e-8
        elif precision == mx.float16:
            rtol = rtol or 3.0e-2
            atol = atol or 1.0e-5
        else:
            raise ValueError(f"Unsupported precision: {precision}")
        assert a.shape == b.shape, f"shape mismatch: {a.shape} vs {b.shape}"
        if not np.allclose(a, b, rtol=rtol, atol=atol):
            diff = np.invert(np.isclose(a, b, rtol=rtol, atol=atol))
            if diff.size > 10000 and np.sum(diff) <= 3:
                # if only a small number of elements are different in a large array, probably fine
                return
            with np.printoptions(precision=3, suppress=True):
                print("a=", a)
                print("b=", b)
                print("diff_a=", a * diff)
                print("diff_b=", b * diff)
                print("diff_a_val=", a[diff])
                print("diff_b_val=", b[diff])
>               assert False, f"result mismatch: {message}"
                       ^^^^^
E               AssertionError: result mismatch: None

tests/utils.py:42: AssertionError
----------------------------- Captured stdout call -----------------------------
a= [[[[[nan nan nan nan nan]
    [nan nan nan nan nan]
    [nan nan nan nan nan]
    [nan nan nan nan nan]]

   [[nan nan nan nan nan]
    [nan nan nan nan nan]
    [nan nan nan nan nan]
    [nan nan nan nan nan]]

   [[nan nan nan nan nan]
    [nan nan nan nan nan]
    [nan nan nan nan nan]
    [nan nan nan nan nan]]]


  [[[nan nan nan nan nan]
    [nan nan nan nan nan]
    [nan nan nan nan nan]
    [nan nan nan nan nan]]

   [[nan nan nan nan nan]
    [nan nan nan nan nan]
    [nan nan nan nan nan]
    [nan nan nan nan nan]]

   [[nan nan nan nan nan]
    [nan nan nan nan nan]
    [nan nan nan nan nan]
    [nan nan nan nan nan]]]


  [[[nan nan nan nan nan]
    [nan nan nan nan nan]
    [nan nan nan nan nan]
    [nan nan nan nan nan]]

   [[nan nan nan nan nan]
    [nan nan nan nan nan]
    [nan nan nan nan nan]
    [nan nan nan nan nan]]

   [[nan nan nan nan nan]
    [nan nan nan nan nan]
    [nan nan nan nan nan]
    [nan nan nan nan nan]]]]



 [[[[nan nan nan nan nan]
    [nan nan nan nan nan]
    [nan nan nan nan nan]
    [nan nan nan nan nan]]

   [[nan nan nan nan nan]
    [nan nan nan nan nan]
    [nan nan nan nan nan]
    [nan nan nan nan nan]]

   [[nan nan nan nan nan]
    [nan nan nan nan nan]
    [nan nan nan nan nan]
    [nan nan nan nan nan]]]


  [[[nan nan nan nan nan]
    [nan nan nan nan nan]
    [nan nan nan nan nan]
    [nan nan nan nan nan]]

   [[nan nan nan nan nan]
    [nan nan nan nan nan]
    [nan nan nan nan nan]
    [nan nan nan nan nan]]

   [[nan nan nan nan nan]
    [nan nan nan nan nan]
    [nan nan nan nan nan]
    [nan nan nan nan nan]]]


  [[[nan nan nan nan nan]
    [nan nan nan nan nan]
    [nan nan nan nan nan]
    [nan nan nan nan nan]]

   [[nan nan nan nan nan]
    [nan nan nan nan nan]
    [nan nan nan nan nan]
    [nan nan nan nan nan]]

   [[nan nan nan nan nan]
    [nan nan nan nan nan]
    [nan nan nan nan nan]
    [nan nan nan nan nan]]]]]
b= [[[[[0.48  0.624 0.35  0.725 0.44 ]
    [0.575 0.688 0.43  0.585 0.467]
    [0.525 0.656 0.473 0.606 0.551]
    [0.579 0.681 0.406 0.59  0.439]]

   [[0.483 0.619 0.724 0.785 0.58 ]
    [0.408 0.629 0.742 0.84  0.571]
    [0.478 0.629 0.717 0.796 0.592]
    [0.465 0.634 0.724 0.805 0.594]]

   [[0.745 0.555 0.353 0.567 0.443]
    [0.725 0.588 0.295 0.596 0.429]
    [0.736 0.542 0.39  0.579 0.45 ]
    [0.696 0.521 0.299 0.662 0.44 ]]]


  [[[0.714 0.416 0.642 0.273 0.315]
    [0.758 0.467 0.715 0.335 0.337]
    [0.702 0.47  0.634 0.28  0.281]
    [0.766 0.445 0.694 0.33  0.359]]

   [[0.61  0.539 0.461 0.6   0.276]
    [0.596 0.573 0.489 0.537 0.311]
    [0.609 0.503 0.489 0.565 0.266]
    [0.648 0.48  0.447 0.597 0.237]]

   [[0.552 0.283 0.422 0.423 0.814]
    [0.503 0.279 0.452 0.447 0.81 ]
    [0.507 0.276 0.446 0.442 0.813]
    [0.565 0.274 0.417 0.457 0.758]]]


  [[[0.471 0.704 0.516 0.34  0.389]
    [0.53  0.727 0.535 0.324 0.36 ]
    [0.487 0.793 0.619 0.256 0.461]
    [0.552 0.755 0.584 0.285 0.368]]

   [[0.496 0.22  0.491 0.298 0.513]
    [0.522 0.26  0.511 0.339 0.604]
    [0.489 0.24  0.506 0.324 0.559]
    [0.485 0.222 0.516 0.236 0.499]]

   [[0.411 0.421 0.292 0.38  0.839]
    [0.398 0.443 0.284 0.361 0.823]
    [0.35  0.401 0.291 0.382 0.826]
    [0.364 0.441 0.299 0.391 0.833]]]]



 [[[[0.262 0.358 0.621 0.339 0.38 ]
    [0.251 0.351 0.655 0.345 0.373]
    [0.286 0.323 0.592 0.346 0.369]
    [0.263 0.322 0.643 0.331 0.396]]

   [[0.387 0.628 0.642 0.626 0.425]
    [0.446 0.614 0.64  0.657 0.478]
    [0.442 0.549 0.658 0.632 0.376]
    [0.478 0.523 0.681 0.652 0.402]]

   [[0.59  0.41  0.502 0.768 0.533]
    [0.55  0.431 0.5   0.764 0.483]
    [0.575 0.472 0.527 0.777 0.487]
    [0.583 0.453 0.523 0.772 0.5  ]]]


  [[[0.502 0.694 0.351 0.508 0.403]
    [0.492 0.679 0.37  0.495 0.398]
    [0.454 0.643 0.415 0.474 0.461]
    [0.485 0.683 0.354 0.528 0.397]]

   [[0.259 0.574 0.561 0.561 0.726]
    [0.25  0.508 0.521 0.561 0.714]
    [0.234 0.6   0.491 0.537 0.729]
    [0.226 0.583 0.536 0.619 0.754]]

   [[0.245 0.573 0.411 0.316 0.552]
    [0.24  0.565 0.435 0.325 0.513]
    [0.207 0.613 0.494 0.35  0.418]
    [0.212 0.663 0.496 0.369 0.425]]]


  [[[0.561 0.542 0.459 0.52  0.402]
    [0.671 0.618 0.36  0.543 0.322]
    [0.533 0.54  0.497 0.549 0.345]
    [0.664 0.604 0.35  0.521 0.371]]

   [[0.269 0.518 0.407 0.49  0.652]
    [0.275 0.551 0.396 0.483 0.645]
    [0.217 0.576 0.428 0.418 0.605]
    [0.271 0.527 0.39  0.447 0.667]]

   [[0.158 0.532 0.691 0.403 0.588]
    [0.201 0.42  0.691 0.355 0.575]
    [0.214 0.456 0.74  0.271 0.648]
    [0.183 0.5   0.708 0.348 0.613]]]]]
diff_a= [[[[[nan nan nan nan nan]
    [nan nan nan nan nan]
    [nan nan nan nan nan]
    [nan nan nan nan nan]]

   [[nan nan nan nan nan]
    [nan nan nan nan nan]
    [nan nan nan nan nan]
    [nan nan nan nan nan]]

   [[nan nan nan nan nan]
    [nan nan nan nan nan]
    [nan nan nan nan nan]
    [nan nan nan nan nan]]]


  [[[nan nan nan nan nan]
    [nan nan nan nan nan]
    [nan nan nan nan nan]
    [nan nan nan nan nan]]

   [[nan nan nan nan nan]
    [nan nan nan nan nan]
    [nan nan nan nan nan]
    [nan nan nan nan nan]]

   [[nan nan nan nan nan]
    [nan nan nan nan nan]
    [nan nan nan nan nan]
    [nan nan nan nan nan]]]


  [[[nan nan nan nan nan]
    [nan nan nan nan nan]
    [nan nan nan nan nan]
    [nan nan nan nan nan]]

   [[nan nan nan nan nan]
    [nan nan nan nan nan]
    [nan nan nan nan nan]
    [nan nan nan nan nan]]

   [[nan nan nan nan nan]
    [nan nan nan nan nan]
    [nan nan nan nan nan]
    [nan nan nan nan nan]]]]



 [[[[nan nan nan nan nan]
    [nan nan nan nan nan]
    [nan nan nan nan nan]
    [nan nan nan nan nan]]

   [[nan nan nan nan nan]
    [nan nan nan nan nan]
    [nan nan nan nan nan]
    [nan nan nan nan nan]]

   [[nan nan nan nan nan]
    [nan nan nan nan nan]
    [nan nan nan nan nan]
    [nan nan nan nan nan]]]


  [[[nan nan nan nan nan]
    [nan nan nan nan nan]
    [nan nan nan nan nan]
    [nan nan nan nan nan]]

   [[nan nan nan nan nan]
    [nan nan nan nan nan]
    [nan nan nan nan nan]
    [nan nan nan nan nan]]

   [[nan nan nan nan nan]
    [nan nan nan nan nan]
    [nan nan nan nan nan]
    [nan nan nan nan nan]]]


  [[[nan nan nan nan nan]
    [nan nan nan nan nan]
    [nan nan nan nan nan]
    [nan nan nan nan nan]]

   [[nan nan nan nan nan]
    [nan nan nan nan nan]
    [nan nan nan nan nan]
    [nan nan nan nan nan]]

   [[nan nan nan nan nan]
    [nan nan nan nan nan]
    [nan nan nan nan nan]
    [nan nan nan nan nan]]]]]
diff_b= [[[[[0.48  0.624 0.35  0.725 0.44 ]
    [0.575 0.688 0.43  0.585 0.467]
    [0.525 0.656 0.473 0.606 0.551]
    [0.579 0.681 0.406 0.59  0.439]]

   [[0.483 0.619 0.724 0.785 0.58 ]
    [0.408 0.629 0.742 0.84  0.571]
    [0.478 0.629 0.717 0.796 0.592]
    [0.465 0.634 0.724 0.805 0.594]]

   [[0.745 0.555 0.353 0.567 0.443]
    [0.725 0.588 0.295 0.596 0.429]
    [0.736 0.542 0.39  0.579 0.45 ]
    [0.696 0.521 0.299 0.662 0.44 ]]]


  [[[0.714 0.416 0.642 0.273 0.315]
    [0.758 0.467 0.715 0.335 0.337]
    [0.702 0.47  0.634 0.28  0.281]
    [0.766 0.445 0.694 0.33  0.359]]

   [[0.61  0.539 0.461 0.6   0.276]
    [0.596 0.573 0.489 0.537 0.311]
    [0.609 0.503 0.489 0.565 0.266]
    [0.648 0.48  0.447 0.597 0.237]]

   [[0.552 0.283 0.422 0.423 0.814]
    [0.503 0.279 0.452 0.447 0.81 ]
    [0.507 0.276 0.446 0.442 0.813]
    [0.565 0.274 0.417 0.457 0.758]]]


  [[[0.471 0.704 0.516 0.34  0.389]
    [0.53  0.727 0.535 0.324 0.36 ]
    [0.487 0.793 0.619 0.256 0.461]
    [0.552 0.755 0.584 0.285 0.368]]

   [[0.496 0.22  0.491 0.298 0.513]
    [0.522 0.26  0.511 0.339 0.604]
    [0.489 0.24  0.506 0.324 0.559]
    [0.485 0.222 0.516 0.236 0.499]]

   [[0.411 0.421 0.292 0.38  0.839]
    [0.398 0.443 0.284 0.361 0.823]
    [0.35  0.401 0.291 0.382 0.826]
    [0.364 0.441 0.299 0.391 0.833]]]]



 [[[[0.262 0.358 0.621 0.339 0.38 ]
    [0.251 0.351 0.655 0.345 0.373]
    [0.286 0.323 0.592 0.346 0.369]
    [0.263 0.322 0.643 0.331 0.396]]

   [[0.387 0.628 0.642 0.626 0.425]
    [0.446 0.614 0.64  0.657 0.478]
    [0.442 0.549 0.658 0.632 0.376]
    [0.478 0.523 0.681 0.652 0.402]]

   [[0.59  0.41  0.502 0.768 0.533]
    [0.55  0.431 0.5   0.764 0.483]
    [0.575 0.472 0.527 0.777 0.487]
    [0.583 0.453 0.523 0.772 0.5  ]]]


  [[[0.502 0.694 0.351 0.508 0.403]
    [0.492 0.679 0.37  0.495 0.398]
    [0.454 0.643 0.415 0.474 0.461]
    [0.485 0.683 0.354 0.528 0.397]]

   [[0.259 0.574 0.561 0.561 0.726]
    [0.25  0.508 0.521 0.561 0.714]
    [0.234 0.6   0.491 0.537 0.729]
    [0.226 0.583 0.536 0.619 0.754]]

   [[0.245 0.573 0.411 0.316 0.552]
    [0.24  0.565 0.435 0.325 0.513]
    [0.207 0.613 0.494 0.35  0.418]
    [0.212 0.663 0.496 0.369 0.425]]]


  [[[0.561 0.542 0.459 0.52  0.402]
    [0.671 0.618 0.36  0.543 0.322]
    [0.533 0.54  0.497 0.549 0.345]
    [0.664 0.604 0.35  0.521 0.371]]

   [[0.269 0.518 0.407 0.49  0.652]
    [0.275 0.551 0.396 0.483 0.645]
    [0.217 0.576 0.428 0.418 0.605]
    [0.271 0.527 0.39  0.447 0.667]]

   [[0.158 0.532 0.691 0.403 0.588]
    [0.201 0.42  0.691 0.355 0.575]
    [0.214 0.456 0.74  0.271 0.648]
    [0.183 0.5   0.708 0.348 0.613]]]]]
diff_a_val= [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan
 nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan
 nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan
 nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan
 nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan
 nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan
 nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan
 nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan
 nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan
 nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan
 nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan
 nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan
 nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan
 nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan
 nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan
 nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan
 nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan
 nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan
 nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan
 nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan]
diff_b_val= [0.48  0.624 0.35  0.725 0.44  0.575 0.688 0.43  0.585 0.467 0.525 0.656
 0.473 0.606 0.551 0.579 0.681 0.406 0.59  0.439 0.483 0.619 0.724 0.785
 0.58  0.408 0.629 0.742 0.84  0.571 0.478 0.629 0.717 0.796 0.592 0.465
 0.634 0.724 0.805 0.594 0.745 0.555 0.353 0.567 0.443 0.725 0.588 0.295
 0.596 0.429 0.736 0.542 0.39  0.579 0.45  0.696 0.521 0.299 0.662 0.44
 0.714 0.416 0.642 0.273 0.315 0.758 0.467 0.715 0.335 0.337 0.702 0.47
 0.634 0.28  0.281 0.766 0.445 0.694 0.33  0.359 0.61  0.539 0.461 0.6
 0.276 0.596 0.573 0.489 0.537 0.311 0.609 0.503 0.489 0.565 0.266 0.648
 0.48  0.447 0.597 0.237 0.552 0.283 0.422 0.423 0.814 0.503 0.279 0.452
 0.447 0.81  0.507 0.276 0.446 0.442 0.813 0.565 0.274 0.417 0.457 0.758
 0.471 0.704 0.516 0.34  0.389 0.53  0.727 0.535 0.324 0.36  0.487 0.793
 0.619 0.256 0.461 0.552 0.755 0.584 0.285 0.368 0.496 0.22  0.491 0.298
 0.513 0.522 0.26  0.511 0.339 0.604 0.489 0.24  0.506 0.324 0.559 0.485
 0.222 0.516 0.236 0.499 0.411 0.421 0.292 0.38  0.839 0.398 0.443 0.284
 0.361 0.823 0.35  0.401 0.291 0.382 0.826 0.364 0.441 0.299 0.391 0.833
 0.262 0.358 0.621 0.339 0.38  0.251 0.351 0.655 0.345 0.373 0.286 0.323
 0.592 0.346 0.369 0.263 0.322 0.643 0.331 0.396 0.387 0.628 0.642 0.626
 0.425 0.446 0.614 0.64  0.657 0.478 0.442 0.549 0.658 0.632 0.376 0.478
 0.523 0.681 0.652 0.402 0.59  0.41  0.502 0.768 0.533 0.55  0.431 0.5
 0.764 0.483 0.575 0.472 0.527 0.777 0.487 0.583 0.453 0.523 0.772 0.5
 0.502 0.694 0.351 0.508 0.403 0.492 0.679 0.37  0.495 0.398 0.454 0.643
 0.415 0.474 0.461 0.485 0.683 0.354 0.528 0.397 0.259 0.574 0.561 0.561
 0.726 0.25  0.508 0.521 0.561 0.714 0.234 0.6   0.491 0.537 0.729 0.226
 0.583 0.536 0.619 0.754 0.245 0.573 0.411 0.316 0.552 0.24  0.565 0.435
 0.325 0.513 0.207 0.613 0.494 0.35  0.418 0.212 0.663 0.496 0.369 0.425
 0.561 0.542 0.459 0.52  0.402 0.671 0.618 0.36  0.543 0.322 0.533 0.54
 0.497 0.549 0.345 0.664 0.604 0.35  0.521 0.371 0.269 0.518 0.407 0.49
 0.652 0.275 0.551 0.396 0.483 0.645 0.217 0.576 0.428 0.418 0.605 0.271
 0.527 0.39  0.447 0.667 0.158 0.532 0.691 0.403 0.588 0.201 0.42  0.691
 0.355 0.575 0.214 0.456 0.74  0.271 0.648 0.183 0.5   0.708 0.348 0.613]
___________ test_task_1_simple_attention_scale_mask[batch_2-f16-gpu] ___________

stream = DeviceType.gpu, precision = mlx.core.float16, batch_dimension = 2

    @pytest.mark.parametrize("stream", AVAILABLE_STREAMS, ids=AVAILABLE_STREAMS_IDS)
    @pytest.mark.parametrize("precision", PRECISIONS, ids=PRECISION_IDS)
    @pytest.mark.parametrize(
        "batch_dimension", [0, 1, 2], ids=["batch_0", "batch_1", "batch_2"]
    )
    def test_task_1_simple_attention_scale_mask(
        stream: mx.Stream, precision: mx.Dtype, batch_dimension: int
    ):
        """
        Test if `scaled_dot_product_attention_simple` can process scale and mask correctly.
        """
        with mx.stream(stream):
            if batch_dimension == 0:
                BATCH_SIZE = ()
            elif batch_dimension == 1:
                BATCH_SIZE = (2, 3)
            elif batch_dimension == 2:
                BATCH_SIZE = (2, 3, 3)
            DIM_L = 4
            DIM_D = 5
            for _ in range(100):
                query = mx.random.uniform(
                    shape=(*BATCH_SIZE, DIM_L, DIM_D), dtype=precision
                )
                key = mx.random.uniform(shape=(*BATCH_SIZE, DIM_L, DIM_D), dtype=precision)
                value = mx.random.uniform(
                    shape=(*BATCH_SIZE, DIM_L, DIM_D), dtype=precision
                )
                mask = mx.random.uniform(shape=(*BATCH_SIZE, DIM_L, DIM_L), dtype=precision)
                scale = 0.5
                reference_output = mx.fast.scaled_dot_product_attention(
                    q=query.reshape(1, -1, DIM_L, DIM_D),
                    k=key.reshape(1, -1, DIM_L, DIM_D),
                    v=value.reshape(1, -1, DIM_L, DIM_D),
                    scale=scale,
                    mask=mask.reshape(1, -1, DIM_L, DIM_L),
                ).reshape(*BATCH_SIZE, DIM_L, DIM_D)
                user_output = scaled_dot_product_attention_simple(
                    query,
                    key,
                    value,
                    scale=scale,
                    mask=mask,
                )
>               assert_allclose(user_output, reference_output, precision=precision)

tests/test_week_1_day_1.py:108: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

a = array([[[[[nan, nan, nan, nan, nan],
          [nan, nan, nan, nan, nan],
          [nan, nan, nan, nan, nan],
       ...[nan, nan, nan, nan, nan],
          [nan, nan, nan, nan, nan],
          [nan, nan, nan, nan, nan]]]]], dtype=float32)
b = array([[[[[0.2223, 0.563 , 0.802 , 0.7188, 0.2178],
          [0.2092, 0.547 , 0.778 , 0.7266, 0.2444],
          [0.2...       [0.716 , 0.682 , 0.684 , 0.4727, 0.626 ],
          [0.718 , 0.6304, 0.677 , 0.4653, 0.614 ]]]]], dtype=float16)
precision = mlx.core.float16, rtol = 0.03, atol = 1e-05, message = None

    def assert_allclose(
        a: mx.array,
        b: mx.array,
        precision: mx.Dtype,
        rtol: float | None = None,
        atol: float | None = None,
        message: str | None = None,
    ):
        a = np.array(a)
        b = np.array(b)
        if precision == mx.float32:
            rtol = rtol or 1.0e-5
            atol = atol or 1.0e-8
        elif precision == mx.float16:
            rtol = rtol or 3.0e-2
            atol = atol or 1.0e-5
        else:
            raise ValueError(f"Unsupported precision: {precision}")
        assert a.shape == b.shape, f"shape mismatch: {a.shape} vs {b.shape}"
        if not np.allclose(a, b, rtol=rtol, atol=atol):
            diff = np.invert(np.isclose(a, b, rtol=rtol, atol=atol))
            if diff.size > 10000 and np.sum(diff) <= 3:
                # if only a small number of elements are different in a large array, probably fine
                return
            with np.printoptions(precision=3, suppress=True):
                print("a=", a)
                print("b=", b)
                print("diff_a=", a * diff)
                print("diff_b=", b * diff)
                print("diff_a_val=", a[diff])
                print("diff_b_val=", b[diff])
>               assert False, f"result mismatch: {message}"
                       ^^^^^
E               AssertionError: result mismatch: None

tests/utils.py:42: AssertionError
----------------------------- Captured stdout call -----------------------------
a= [[[[[nan nan nan nan nan]
    [nan nan nan nan nan]
    [nan nan nan nan nan]
    [nan nan nan nan nan]]

   [[nan nan nan nan nan]
    [nan nan nan nan nan]
    [nan nan nan nan nan]
    [nan nan nan nan nan]]

   [[nan nan nan nan nan]
    [nan nan nan nan nan]
    [nan nan nan nan nan]
    [nan nan nan nan nan]]]


  [[[nan nan nan nan nan]
    [nan nan nan nan nan]
    [nan nan nan nan nan]
    [nan nan nan nan nan]]

   [[nan nan nan nan nan]
    [nan nan nan nan nan]
    [nan nan nan nan nan]
    [nan nan nan nan nan]]

   [[nan nan nan nan nan]
    [nan nan nan nan nan]
    [nan nan nan nan nan]
    [nan nan nan nan nan]]]


  [[[nan nan nan nan nan]
    [nan nan nan nan nan]
    [nan nan nan nan nan]
    [nan nan nan nan nan]]

   [[nan nan nan nan nan]
    [nan nan nan nan nan]
    [nan nan nan nan nan]
    [nan nan nan nan nan]]

   [[nan nan nan nan nan]
    [nan nan nan nan nan]
    [nan nan nan nan nan]
    [nan nan nan nan nan]]]]



 [[[[nan nan nan nan nan]
    [nan nan nan nan nan]
    [nan nan nan nan nan]
    [nan nan nan nan nan]]

   [[nan nan nan nan nan]
    [nan nan nan nan nan]
    [nan nan nan nan nan]
    [nan nan nan nan nan]]

   [[nan nan nan nan nan]
    [nan nan nan nan nan]
    [nan nan nan nan nan]
    [nan nan nan nan nan]]]


  [[[nan nan nan nan nan]
    [nan nan nan nan nan]
    [nan nan nan nan nan]
    [nan nan nan nan nan]]

   [[nan nan nan nan nan]
    [nan nan nan nan nan]
    [nan nan nan nan nan]
    [nan nan nan nan nan]]

   [[nan nan nan nan nan]
    [nan nan nan nan nan]
    [nan nan nan nan nan]
    [nan nan nan nan nan]]]


  [[[nan nan nan nan nan]
    [nan nan nan nan nan]
    [nan nan nan nan nan]
    [nan nan nan nan nan]]

   [[nan nan nan nan nan]
    [nan nan nan nan nan]
    [nan nan nan nan nan]
    [nan nan nan nan nan]]

   [[nan nan nan nan nan]
    [nan nan nan nan nan]
    [nan nan nan nan nan]
    [nan nan nan nan nan]]]]]
b= [[[[[0.222 0.563 0.802 0.719 0.218]
    [0.209 0.547 0.778 0.727 0.244]
    [0.26  0.563 0.806 0.723 0.246]
    [0.282 0.561 0.81  0.705 0.25 ]]

   [[0.281 0.308 0.52  0.342 0.49 ]
    [0.307 0.29  0.53  0.383 0.41 ]
    [0.296 0.31  0.484 0.353 0.502]
    [0.295 0.308 0.509 0.368 0.42 ]]

   [[0.485 0.686 0.565 0.544 0.667]
    [0.483 0.645 0.555 0.492 0.646]
    [0.423 0.749 0.512 0.474 0.708]
    [0.53  0.656 0.533 0.462 0.639]]]


  [[[0.801 0.431 0.45  0.524 0.82 ]
    [0.818 0.437 0.475 0.453 0.81 ]
    [0.821 0.408 0.38  0.563 0.816]
    [0.778 0.432 0.421 0.588 0.79 ]]

   [[0.481 0.696 0.572 0.324 0.566]
    [0.612 0.655 0.681 0.374 0.433]
    [0.453 0.707 0.568 0.315 0.58 ]
    [0.588 0.651 0.675 0.395 0.484]]

   [[0.398 0.48  0.39  0.454 0.291]
    [0.472 0.476 0.423 0.554 0.339]
    [0.412 0.503 0.388 0.59  0.407]
    [0.393 0.349 0.453 0.592 0.368]]]


  [[[0.633 0.702 0.422 0.419 0.406]
    [0.574 0.704 0.453 0.507 0.519]
    [0.621 0.69  0.438 0.492 0.443]
    [0.575 0.69  0.453 0.557 0.521]]

   [[0.555 0.667 0.484 0.508 0.507]
    [0.448 0.624 0.522 0.492 0.458]
    [0.555 0.649 0.561 0.462 0.483]
    [0.486 0.608 0.528 0.49  0.465]]

   [[0.45  0.602 0.348 0.369 0.376]
    [0.427 0.599 0.339 0.398 0.326]
    [0.439 0.616 0.362 0.353 0.466]
    [0.461 0.625 0.4   0.326 0.503]]]]



 [[[[0.532 0.622 0.433 0.5   0.265]
    [0.581 0.485 0.411 0.489 0.289]
    [0.501 0.56  0.442 0.458 0.228]
    [0.519 0.469 0.437 0.427 0.228]]

   [[0.367 0.529 0.658 0.699 0.766]
    [0.359 0.571 0.734 0.672 0.825]
    [0.36  0.502 0.636 0.678 0.752]
    [0.271 0.59  0.707 0.543 0.839]]

   [[0.406 0.852 0.627 0.525 0.533]
    [0.333 0.758 0.697 0.599 0.471]
    [0.359 0.787 0.645 0.57  0.494]
    [0.396 0.827 0.665 0.578 0.54 ]]]


  [[[0.402 0.445 0.463 0.391 0.415]
    [0.407 0.497 0.482 0.427 0.389]
    [0.369 0.484 0.528 0.476 0.341]
    [0.344 0.442 0.527 0.478 0.343]]

   [[0.337 0.249 0.413 0.648 0.716]
    [0.354 0.25  0.413 0.691 0.705]
    [0.343 0.203 0.393 0.633 0.775]
    [0.356 0.238 0.398 0.679 0.726]]

   [[0.461 0.39  0.395 0.705 0.49 ]
    [0.456 0.43  0.462 0.687 0.491]
    [0.42  0.486 0.388 0.683 0.443]
    [0.504 0.454 0.45  0.723 0.431]]]


  [[[0.354 0.596 0.559 0.552 0.553]
    [0.427 0.52  0.43  0.635 0.544]
    [0.395 0.542 0.467 0.585 0.55 ]
    [0.415 0.534 0.557 0.609 0.592]]

   [[0.481 0.285 0.623 0.493 0.362]
    [0.406 0.352 0.623 0.565 0.379]
    [0.41  0.33  0.727 0.63  0.426]
    [0.421 0.363 0.672 0.546 0.429]]

   [[0.715 0.627 0.68  0.486 0.627]
    [0.676 0.662 0.642 0.452 0.564]
    [0.716 0.682 0.684 0.473 0.626]
    [0.718 0.63  0.677 0.465 0.614]]]]]
diff_a= [[[[[nan nan nan nan nan]
    [nan nan nan nan nan]
    [nan nan nan nan nan]
    [nan nan nan nan nan]]

   [[nan nan nan nan nan]
    [nan nan nan nan nan]
    [nan nan nan nan nan]
    [nan nan nan nan nan]]

   [[nan nan nan nan nan]
    [nan nan nan nan nan]
    [nan nan nan nan nan]
    [nan nan nan nan nan]]]


  [[[nan nan nan nan nan]
    [nan nan nan nan nan]
    [nan nan nan nan nan]
    [nan nan nan nan nan]]

   [[nan nan nan nan nan]
    [nan nan nan nan nan]
    [nan nan nan nan nan]
    [nan nan nan nan nan]]

   [[nan nan nan nan nan]
    [nan nan nan nan nan]
    [nan nan nan nan nan]
    [nan nan nan nan nan]]]


  [[[nan nan nan nan nan]
    [nan nan nan nan nan]
    [nan nan nan nan nan]
    [nan nan nan nan nan]]

   [[nan nan nan nan nan]
    [nan nan nan nan nan]
    [nan nan nan nan nan]
    [nan nan nan nan nan]]

   [[nan nan nan nan nan]
    [nan nan nan nan nan]
    [nan nan nan nan nan]
    [nan nan nan nan nan]]]]



 [[[[nan nan nan nan nan]
    [nan nan nan nan nan]
    [nan nan nan nan nan]
    [nan nan nan nan nan]]

   [[nan nan nan nan nan]
    [nan nan nan nan nan]
    [nan nan nan nan nan]
    [nan nan nan nan nan]]

   [[nan nan nan nan nan]
    [nan nan nan nan nan]
    [nan nan nan nan nan]
    [nan nan nan nan nan]]]


  [[[nan nan nan nan nan]
    [nan nan nan nan nan]
    [nan nan nan nan nan]
    [nan nan nan nan nan]]

   [[nan nan nan nan nan]
    [nan nan nan nan nan]
    [nan nan nan nan nan]
    [nan nan nan nan nan]]

   [[nan nan nan nan nan]
    [nan nan nan nan nan]
    [nan nan nan nan nan]
    [nan nan nan nan nan]]]


  [[[nan nan nan nan nan]
    [nan nan nan nan nan]
    [nan nan nan nan nan]
    [nan nan nan nan nan]]

   [[nan nan nan nan nan]
    [nan nan nan nan nan]
    [nan nan nan nan nan]
    [nan nan nan nan nan]]

   [[nan nan nan nan nan]
    [nan nan nan nan nan]
    [nan nan nan nan nan]
    [nan nan nan nan nan]]]]]
diff_b= [[[[[0.222 0.563 0.802 0.719 0.218]
    [0.209 0.547 0.778 0.727 0.244]
    [0.26  0.563 0.806 0.723 0.246]
    [0.282 0.561 0.81  0.705 0.25 ]]

   [[0.281 0.308 0.52  0.342 0.49 ]
    [0.307 0.29  0.53  0.383 0.41 ]
    [0.296 0.31  0.484 0.353 0.502]
    [0.295 0.308 0.509 0.368 0.42 ]]

   [[0.485 0.686 0.565 0.544 0.667]
    [0.483 0.645 0.555 0.492 0.646]
    [0.423 0.749 0.512 0.474 0.708]
    [0.53  0.656 0.533 0.462 0.639]]]


  [[[0.801 0.431 0.45  0.524 0.82 ]
    [0.818 0.437 0.475 0.453 0.81 ]
    [0.821 0.408 0.38  0.563 0.816]
    [0.778 0.432 0.421 0.588 0.79 ]]

   [[0.481 0.696 0.572 0.324 0.566]
    [0.612 0.655 0.681 0.374 0.433]
    [0.453 0.707 0.568 0.315 0.58 ]
    [0.588 0.651 0.675 0.395 0.484]]

   [[0.398 0.48  0.39  0.454 0.291]
    [0.472 0.476 0.423 0.554 0.339]
    [0.412 0.503 0.388 0.59  0.407]
    [0.393 0.349 0.453 0.592 0.368]]]


  [[[0.633 0.702 0.422 0.419 0.406]
    [0.574 0.704 0.453 0.507 0.519]
    [0.621 0.69  0.438 0.492 0.443]
    [0.575 0.69  0.453 0.557 0.521]]

   [[0.555 0.667 0.484 0.508 0.507]
    [0.448 0.624 0.522 0.492 0.458]
    [0.555 0.649 0.561 0.462 0.483]
    [0.486 0.608 0.528 0.49  0.465]]

   [[0.45  0.602 0.348 0.369 0.376]
    [0.427 0.599 0.339 0.398 0.326]
    [0.439 0.616 0.362 0.353 0.466]
    [0.461 0.625 0.4   0.326 0.503]]]]



 [[[[0.532 0.622 0.433 0.5   0.265]
    [0.581 0.485 0.411 0.489 0.289]
    [0.501 0.56  0.442 0.458 0.228]
    [0.519 0.469 0.437 0.427 0.228]]

   [[0.367 0.529 0.658 0.699 0.766]
    [0.359 0.571 0.734 0.672 0.825]
    [0.36  0.502 0.636 0.678 0.752]
    [0.271 0.59  0.707 0.543 0.839]]

   [[0.406 0.852 0.627 0.525 0.533]
    [0.333 0.758 0.697 0.599 0.471]
    [0.359 0.787 0.645 0.57  0.494]
    [0.396 0.827 0.665 0.578 0.54 ]]]


  [[[0.402 0.445 0.463 0.391 0.415]
    [0.407 0.497 0.482 0.427 0.389]
    [0.369 0.484 0.528 0.476 0.341]
    [0.344 0.442 0.527 0.478 0.343]]

   [[0.337 0.249 0.413 0.648 0.716]
    [0.354 0.25  0.413 0.691 0.705]
    [0.343 0.203 0.393 0.633 0.775]
    [0.356 0.238 0.398 0.679 0.726]]

   [[0.461 0.39  0.395 0.705 0.49 ]
    [0.456 0.43  0.462 0.687 0.491]
    [0.42  0.486 0.388 0.683 0.443]
    [0.504 0.454 0.45  0.723 0.431]]]


  [[[0.354 0.596 0.559 0.552 0.553]
    [0.427 0.52  0.43  0.635 0.544]
    [0.395 0.542 0.467 0.585 0.55 ]
    [0.415 0.534 0.557 0.609 0.592]]

   [[0.481 0.285 0.623 0.493 0.362]
    [0.406 0.352 0.623 0.565 0.379]
    [0.41  0.33  0.727 0.63  0.426]
    [0.421 0.363 0.672 0.546 0.429]]

   [[0.715 0.627 0.68  0.486 0.627]
    [0.676 0.662 0.642 0.452 0.564]
    [0.716 0.682 0.684 0.473 0.626]
    [0.718 0.63  0.677 0.465 0.614]]]]]
diff_a_val= [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan
 nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan
 nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan
 nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan
 nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan
 nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan
 nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan
 nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan
 nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan
 nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan
 nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan
 nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan
 nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan
 nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan
 nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan
 nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan
 nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan
 nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan
 nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan
 nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan]
diff_b_val= [0.222 0.563 0.802 0.719 0.218 0.209 0.547 0.778 0.727 0.244 0.26  0.563
 0.806 0.723 0.246 0.282 0.561 0.81  0.705 0.25  0.281 0.308 0.52  0.342
 0.49  0.307 0.29  0.53  0.383 0.41  0.296 0.31  0.484 0.353 0.502 0.295
 0.308 0.509 0.368 0.42  0.485 0.686 0.565 0.544 0.667 0.483 0.645 0.555
 0.492 0.646 0.423 0.749 0.512 0.474 0.708 0.53  0.656 0.533 0.462 0.639
 0.801 0.431 0.45  0.524 0.82  0.818 0.437 0.475 0.453 0.81  0.821 0.408
 0.38  0.563 0.816 0.778 0.432 0.421 0.588 0.79  0.481 0.696 0.572 0.324
 0.566 0.612 0.655 0.681 0.374 0.433 0.453 0.707 0.568 0.315 0.58  0.588
 0.651 0.675 0.395 0.484 0.398 0.48  0.39  0.454 0.291 0.472 0.476 0.423
 0.554 0.339 0.412 0.503 0.388 0.59  0.407 0.393 0.349 0.453 0.592 0.368
 0.633 0.702 0.422 0.419 0.406 0.574 0.704 0.453 0.507 0.519 0.621 0.69
 0.438 0.492 0.443 0.575 0.69  0.453 0.557 0.521 0.555 0.667 0.484 0.508
 0.507 0.448 0.624 0.522 0.492 0.458 0.555 0.649 0.561 0.462 0.483 0.486
 0.608 0.528 0.49  0.465 0.45  0.602 0.348 0.369 0.376 0.427 0.599 0.339
 0.398 0.326 0.439 0.616 0.362 0.353 0.466 0.461 0.625 0.4   0.326 0.503
 0.532 0.622 0.433 0.5   0.265 0.581 0.485 0.411 0.489 0.289 0.501 0.56
 0.442 0.458 0.228 0.519 0.469 0.437 0.427 0.228 0.367 0.529 0.658 0.699
 0.766 0.359 0.571 0.734 0.672 0.825 0.36  0.502 0.636 0.678 0.752 0.271
 0.59  0.707 0.543 0.839 0.406 0.852 0.627 0.525 0.533 0.333 0.758 0.697
 0.599 0.471 0.359 0.787 0.645 0.57  0.494 0.396 0.827 0.665 0.578 0.54
 0.402 0.445 0.463 0.391 0.415 0.407 0.497 0.482 0.427 0.389 0.369 0.484
 0.528 0.476 0.341 0.344 0.442 0.527 0.478 0.343 0.337 0.249 0.413 0.648
 0.716 0.354 0.25  0.413 0.691 0.705 0.343 0.203 0.393 0.633 0.775 0.356
 0.238 0.398 0.679 0.726 0.461 0.39  0.395 0.705 0.49  0.456 0.43  0.462
 0.687 0.491 0.42  0.486 0.388 0.683 0.443 0.504 0.454 0.45  0.723 0.431
 0.354 0.596 0.559 0.552 0.553 0.427 0.52  0.43  0.635 0.544 0.395 0.542
 0.467 0.585 0.55  0.415 0.534 0.557 0.609 0.592 0.481 0.285 0.623 0.493
 0.362 0.406 0.352 0.623 0.565 0.379 0.41  0.33  0.727 0.63  0.426 0.421
 0.363 0.672 0.546 0.429 0.715 0.627 0.68  0.486 0.627 0.676 0.662 0.642
 0.452 0.564 0.716 0.682 0.684 0.473 0.626 0.718 0.63  0.677 0.465 0.614]
=========================== short test summary info ============================
FAILED tests/test_week_1_day_1.py::test_task_1_simple_attention[batch_0-f32-cpu]
FAILED tests/test_week_1_day_1.py::test_task_1_simple_attention[batch_0-f32-gpu]
FAILED tests/test_week_1_day_1.py::test_task_1_simple_attention[batch_0-f16-cpu]
FAILED tests/test_week_1_day_1.py::test_task_1_simple_attention[batch_0-f16-gpu]
FAILED tests/test_week_1_day_1.py::test_task_1_simple_attention[batch_1-f32-cpu]
FAILED tests/test_week_1_day_1.py::test_task_1_simple_attention[batch_1-f32-gpu]
FAILED tests/test_week_1_day_1.py::test_task_1_simple_attention[batch_1-f16-cpu]
FAILED tests/test_week_1_day_1.py::test_task_1_simple_attention[batch_1-f16-gpu]
FAILED tests/test_week_1_day_1.py::test_task_1_simple_attention[batch_2-f32-cpu]
FAILED tests/test_week_1_day_1.py::test_task_1_simple_attention[batch_2-f32-gpu]
FAILED tests/test_week_1_day_1.py::test_task_1_simple_attention[batch_2-f16-cpu]
FAILED tests/test_week_1_day_1.py::test_task_1_simple_attention[batch_2-f16-gpu]
FAILED tests/test_week_1_day_1.py::test_task_1_simple_attention_scale_mask[batch_0-f32-cpu]
FAILED tests/test_week_1_day_1.py::test_task_1_simple_attention_scale_mask[batch_0-f32-gpu]
FAILED tests/test_week_1_day_1.py::test_task_1_simple_attention_scale_mask[batch_0-f16-cpu]
FAILED tests/test_week_1_day_1.py::test_task_1_simple_attention_scale_mask[batch_0-f16-gpu]
FAILED tests/test_week_1_day_1.py::test_task_1_simple_attention_scale_mask[batch_1-f32-cpu]
FAILED tests/test_week_1_day_1.py::test_task_1_simple_attention_scale_mask[batch_1-f32-gpu]
FAILED tests/test_week_1_day_1.py::test_task_1_simple_attention_scale_mask[batch_1-f16-cpu]
FAILED tests/test_week_1_day_1.py::test_task_1_simple_attention_scale_mask[batch_1-f16-gpu]
FAILED tests/test_week_1_day_1.py::test_task_1_simple_attention_scale_mask[batch_2-f32-cpu]
FAILED tests/test_week_1_day_1.py::test_task_1_simple_attention_scale_mask[batch_2-f32-gpu]
FAILED tests/test_week_1_day_1.py::test_task_1_simple_attention_scale_mask[batch_2-f16-cpu]
FAILED tests/test_week_1_day_1.py::test_task_1_simple_attention_scale_mask[batch_2-f16-gpu]
================== 24 failed, 4 passed, 8 deselected in 1.47s ==================
